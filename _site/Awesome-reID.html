

<!doctype html>
<html lang="en" class="no-js">
  <head>
	<meta name="viewport" content="width=device-width, initial-scale=1">
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Awesome reID - Zhedong Zheng</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Zhedong Zheng">
<meta property="og:title" content="Awesome reID">








  <link rel="canonical" href="https://zdzheng.xyz/Awesome-reID">
  <meta property="og:url" content="https://zdzheng.xyz/Awesome-reID">























<!-- end SEO -->


<link href="https://zdzheng.xyz/feed.xml" type="application/atom+xml" rel="alternate" title="Zhedong Zheng Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<!--
<meta name="viewport" content="width=device-width, initial-scale=1.0">
-->
<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/main.css">

<!-- favicon -->
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">

<meta http-equiv="cleartype" content="on">

    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://zdzheng.xyz/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://zdzheng.xyz/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://zdzheng.xyz/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://zdzheng.xyz/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://zdzheng.xyz/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://zdzheng.xyz/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://zdzheng.xyz/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://zdzheng.xyz/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://zdzheng.xyz/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="mask-icon" href="https://zdzheng.xyz/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://zdzheng.xyz/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://zdzheng.xyz/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://zdzheng.xyz/">Zhedong Zheng</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/">Home</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/research">Research</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/files/zhedong-resume.pdf">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/seminar/">Seminar</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/recruitment/">Join Us</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





 
	<style>
		.author {
			text-decoration: none !important;
			color: #333333;
		}

		.author:hover {
			text-decoration: underline;
			color: #0066cc;
		}
	    pre {
	      background-color: gray;
	      color: white;
	      padding: 10px;
	      border-radius: 5px;
	    }
	</style>


<div id="main" role="main">
  <meta name="og:description" content="Awesome reID"> 
  



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Awesome reID">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Awesome reID
</h1>
          
        
        
        
        

        
		
		
		
		
			
		
		  <meta name="description" content="Awesome reID">
		
		
		
		
		
		
		
		
		
    
        </header>
      

      <section class="page__content" itemprop="text">
        <style>
table, th, td {
  border: 1px solid black;
}
</style>

<p>If you notice any result or the public code that has not been included in this table, please connect <a href="mailto:zdzheng12@gmail.com">Zhedong Zheng</a> without hesitation to add the method. You are welcomed! 
or create pull request.</p>

<p>Priorities are given to papers whose codes are published.</p>

<ul>
  <li><a href="#supervised-learning">Supervised Learning</a></li>
  <li><a href="#transfer-learning">Transfer Learning</a></li>
</ul>

<h3 id="code">Code</h3>

<p><img class="emoji" title=":red_car:" alt=":red_car:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f697.png" height="20" width="20">  The 1st Place Submission to AICity Challenge 2020 re-id track <a href="https://github.com/layumi/AICIty-reID-2020">[code]</a>
<a href="https://github.com/layumi/AICIty-reID-2020/blob/master/paper.pdf">[paper]</a></p>

<p><img class="emoji" title=":helicopter:" alt=":helicopter:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f681.png" height="20" width="20">  Drone-based building re-id <a href="https://github.com/layumi/University1652-Baseline">[code]</a>  <a href="https://arxiv.org/abs/2002.12186">[paper]</a></p>

<h2 id="supervised-learning">Supervised Learning</h2>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Rank@1</th>
      <th>mAP</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BoW+kissme</td>
      <td>25.13%</td>
      <td>12.17%</td>
      <td>“<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410490">Scalable person re-identification: a benchmark</a>”, Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang and Qi Tian, ICCV 2015 <a href="http://www.liangzheng.org/Project/project_reid.html"><strong>[project]</strong></a>
</td>
    </tr>
    <tr>
      <td>LOMO+XQDA</td>
      <td>30.75%</td>
      <td>17.04%</td>
      <td>“<a href="https://arxiv.org/abs/1406.4216">Person Re-identification by Local Maximal Occurrence Representation and Metric Learning</a>”, Shengcai Liao, Yang Hu, Xiangyu Zhu and Stan Z Li, CVPR 2015 <a href="http://www.cbsr.ia.ac.cn/users/scliao/projects/lomo_xqda/index.html"><strong>[project]</strong></a>
</td>
    </tr>
    <tr>
      <td>Basel.</td>
      <td>65.22%</td>
      <td>44.99%</td>
      <td>“<a href="https://arxiv.org/abs/1610.02984">Person Re-identification: Past, Present and Future</a>”, Liang Zheng, Yi Yang, and Alexander G. Hauptmann, arXiv:1610.02984 <a href="https://github.com/zhunzhong07/IDE-baseline-Market-1501"><strong>[code]</strong></a>
</td>
    </tr>
    <tr>
      <td>Basel. + LSRO  </td>
      <td>67.68%</td>
      <td>47.13%</td>
      <td>“<a href="https://arxiv.org/abs/1701.07717">Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</a>”, Zhedong Zheng, Liang Zheng and Yi Yang, ICCV 2017 <a href="https://github.com/layumi/Person-reID_GAN"><strong>[code]</strong></a>
</td>
    </tr>
    <tr>
      <td>Basel. + OIM</td>
      <td>68.1%</td>
      <td>-</td>
      <td>“<a href="https://arxiv.org/abs/1604.01850">Joint Detection and Identification Feature Learning for Person Search</a>”, Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, Xiaogang Wang, CVPR 2017</td>
    </tr>
    <tr>
      <td>Verif + Identif</td>
      <td>68.9%</td>
      <td>49.3%</td>
      <td>“<a href="https://arxiv.org/abs/1611.05666">A Discriminatively Learned Cnn Embedding for Person Re-identification</a>”,  Zhedong Zheng, Liang Zheng, and Yi Yang, TOMM 2017. <a href="https://github.com/layumi/2016_person_re-ID"><strong>[code]</strong></a>
</td>
    </tr>
    <tr>
      <td>APR</td>
      <td>70.69%</td>
      <td>51.88%</td>
      <td>“<a href="https://arxiv.org/abs/1703.07220">Improving person re-identification by attribute and identity learning</a>”, Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu, Yi Yang, Pattern Recognition 2019 <a href="https://github.com/vana77/DukeMTMC-attribute">[Attribute Dataset]</a>
</td>
    </tr>
    <tr>
      <td>ACRN</td>
      <td>72.58%</td>
      <td>51.96%</td>
      <td>“<a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w17/papers/Schumann_Person_Re-Identification_by_CVPR_2017_paper.pdf">Person Re-Identification by Deep Learning Attribute-Complementary Information</a>”, Arne Schumann and Rainer Stiefelhagen, CVPR 2017 Workshop</td>
    </tr>
    <tr>
      <td>PAN</td>
      <td>71.59%</td>
      <td>51.51%</td>
      <td>“<a href="https://arxiv.org/abs/1707.00408">Pedestrian Alignment Network for Large-scale Person Re-identification</a>”, Zhedong Zheng, Liang Zheng, Yi Yang, TCSVT 2018 <a href="https://github.com/layumi/Pedestrian_Alignment"><strong>[code]</strong></a>
</td>
    </tr>
    <tr>
      <td>PAN+rerank</td>
      <td>75.94%</td>
      <td>66.74%</td>
      <td> </td>
    </tr>
    <tr>
      <td>FMN</td>
      <td>74.51%</td>
      <td>56.88%</td>
      <td>“<a href="https://arxiv.org/abs/1711.07155">Let Features Decide for Themselves: Feature Mask Network for Person Re-identification</a>”, Guodong Ding, Salman Khan, Zhenmin Tang, Fatih Porikli, arXiv:1711.07155</td>
    </tr>
    <tr>
      <td>FMN+rerank</td>
      <td>79.52%</td>
      <td>72.79%</td>
      <td> </td>
    </tr>
    <tr>
      <td>Bilinear Coding</td>
      <td>76.2%</td>
      <td>56.9%</td>
      <td>“<a href="https://arxiv.org/abs/1803.08580">Weighted Bilinear Coding over Salient Body Parts for Person Re-identification</a>” Zhigang Chang, Zhou Qin, Heng Fan, Hang Su, Hua Yang, Shibao Zheng, and Haibin Ling, Neurocomputing</td>
    </tr>
    <tr>
      <td>SVDNet</td>
      <td>76.7%</td>
      <td>56.8%</td>
      <td>“<a href="https://arxiv.org/abs/1703.05693">SVDNet for Pedestrian Retrieval</a>”, Yifan Sun, Liang Zheng, Weijian Deng, Shengjin Wang, ICCV 2017 <a href="https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval">[code]</a>
</td>
    </tr>
    <tr>
      <td>OG-Net</td>
      <td>76.93%</td>
      <td>57.20%</td>
      <td>“<a href="https://arxiv.org/abs/2006.04569">Parameter-Efficient Person Re-identification in the 3D Space</a>”,  Zhedong Zheng and Yi Yang, TNNLS 2022. <a href="https://github.com/layumi/person-reid-3d"><strong>[pytorch code]</strong></a>
</td>
    </tr>
    <tr>
      <td>dMpRL</td>
      <td>76.81%</td>
      <td>58.56%</td>
      <td>“<a href="https://arxiv.org/abs/1801.06742">Multi-pseudo Regularized Label for Generated Samples in Person Re-Identification</a>”, Huang Yan, Jinsong Xu, Qiang Wu, Zhedong Zheng, Zhaoxiang Zhang, and Jian Zhang, TIP 2018 <a href="https://github.com/Huang-3/MpRL-for-person-re-ID">[code]</a>
</td>
    </tr>
    <tr>
      <td>AACN</td>
      <td>76.84%</td>
      <td>59.25%</td>
      <td>“<a href="https://arxiv.org/abs/1805.03344">Attention-Aware Compositional Network for Person Re-identification</a>”, Jing Xu, Rui Zhao, Feng Zhu, Huaming Wang and Wanli Ouyang, CVPR2018</td>
    </tr>
    <tr>
      <td>CamStyle + RE</td>
      <td>78.32%</td>
      <td>57.61%</td>
      <td>“<a href="https://arxiv.org/abs/1711.10295">Camera Style Adaptation for Person Re-identification</a>”, Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang, CVPR 2018 <a href="https://github.com/layumi/Pedestrian_Alignment"><strong>[code]</strong></a>
</td>
    </tr>
    <tr>
      <td>DPFL</td>
      <td>79.2%</td>
      <td>60.6%</td>
      <td>“<a href="http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf">Person Re-Identification by Deep Learning Multi-Scale Representations</a>”, Yanbei Chen, Xiatian Zhu and Shaogang Gong, ICCV2017 workshop</td>
    </tr>
    <tr>
      <td>SVDNet + RE</td>
      <td>79.31%</td>
      <td>62.44%</td>
      <td>“<a href="https://arxiv.org/abs/1708.04896">Random Erasing Data Augmentation</a>”, Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, AAAI 2020</td>
    </tr>
    <tr>
      <td>SVDNet + RE + rerank</td>
      <td>84.02%</td>
      <td>78.28%</td>
      <td> </td>
    </tr>
    <tr>
      <td>PSE</td>
      <td>79.8%</td>
      <td>62.0%</td>
      <td>“<a href="https://arxiv.org/abs/1711.10378">A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</a>”, M. Saquib Sarfraz, Arne Schumann, Andreas Eberle, Rainer Stiefelhagen, CVPR 2018<a href="https://github.com/pse-ecn/pose-sensitive-embedding"><strong>[code]</strong></a>
</td>
    </tr>
    <tr>
      <td>PSE + ECN + rerank</td>
      <td>85.2%</td>
      <td>79.8%</td>
      <td> </td>
    </tr>
    <tr>
      <td>ATWL(2-stream)</td>
      <td>79.80%</td>
      <td>63.40%</td>
      <td>“<a href="https://arxiv.org/abs/1803.10859">Features for Multi-Target Multi-Camera Tracking and Re-Identification</a>”, Ergys Ristani and Carlo Tomasi, CVPR 2018</td>
    </tr>
    <tr>
      <td>Mid-level Representation</td>
      <td>80.43%</td>
      <td>63.88%</td>
      <td>“<a href="https://arxiv.org/abs/1711.08106">The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching</a>”, Qian Yu, Xiaobin Chang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales, arXiv:1711.08106</td>
    </tr>
    <tr>
      <td>HA-CNN</td>
      <td>80.5%</td>
      <td>63.8%</td>
      <td>“<a href="https://arxiv.org/abs/1802.08122">Harmonious Attention Network for Person Re-Identification</a>”, Li Wei, Xiatian Zhu, and Shaogang Gong, CVPR 2018</td>
    </tr>
    <tr>
      <td>Deep-Person</td>
      <td>80.90%</td>
      <td>64.80%</td>
      <td>“<a href="https://arxiv.org/abs/1711.10658">Deep-Person: Learning Discriminative Deep Features for Person Re-Identification</a>”, Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, Yongchao Xu, arXiv:1711.10658</td>
    </tr>
    <tr>
      <td>MLFN</td>
      <td>81.2%</td>
      <td>62.8%</td>
      <td>“<a href="https://arxiv.org/abs/1803.09132">Multi-Level Factorisation Net for Person Re-Identification</a>” Xiaobin Chang, Timothy M. Hospedales, and Tao Xiang, CVPR 2018.</td>
    </tr>
    <tr>
      <td>DuATM (Dense-121)</td>
      <td>81.82%</td>
      <td>64.58%</td>
      <td>“<a href="https://arxiv.org/abs/1803.09937">Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification</a>”, Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex C. Kot, Gang Wang, CVPR 2018</td>
    </tr>
    <tr>
      <td>PCB</td>
      <td>83.3%</td>
      <td>69.2%</td>
      <td>“<a href="https://arxiv.org/abs/1711.09349">Beyond Part Models: Person Retrieval with Refined Part Pooling</a>”, Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang, ECCV 2018</td>
    </tr>
    <tr>
      <td>Part-aligned（Inception V1, OpenPose)</td>
      <td>84.4%</td>
      <td>69.3%</td>
      <td>“<a href="https://arxiv.org/abs/1804.07094">Part-Aligned Bilinear Representations for Person Re-identification</a>”, Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, Kyoung Mu Lee, ECCV 2018</td>
    </tr>
    <tr>
      <td>GP-reID</td>
      <td>85.2%</td>
      <td>72.8%</td>
      <td>“<a href="https://arxiv.org/abs/1801.05339">Re-ID done right: towards good practices for person re-identification</a>”, Jon Almazan, Bojana Gajic, Naila Murray, Diane Larlus, arXiv:1801.05339</td>
    </tr>
    <tr>
      <td>SPreID (Res-152)</td>
      <td>85.95%</td>
      <td>73.34%</td>
      <td>“<a href="https://arxiv.org/abs/1804.00216">Human Semantic Parsing for Person Re-identification</a>”, Kalayeh, Mahdi M., Emrah Basaran, Muhittin Gokmen, Mustafa E. Kamasak, and Mubarak Shah, CVPR 2018</td>
    </tr>
    <tr>
      <td>DG-Net (Res-50)</td>
      <td>86.6%</td>
      <td>74.8%</td>
      <td>“<a href="https://arxiv.org/abs/1904.07223">Joint Discriminative and Generative Learning for Person Re-identification</a>”,  Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang and Jan Kautz, CVPR 2019. <a href="https://github.com/NVlabs/DG-Net">[code]</a>
</td>
    </tr>
    <tr>
      <td>MGN</td>
      <td>88.7%</td>
      <td>78.4%</td>
      <td>“<a href="https://arxiv.org/abs/1804.01438">Learning Discriminative Features with Multiple Granularities for Person Re-Identification</a>” Wang, Guanshuo, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou. ACM MM 2018.</td>
    </tr>
  </tbody>
</table>

<h2 id="transfer-learning">Transfer Learning</h2>
<ul>
  <li>Train on <a href="http://www.liangzheng.com.cn/Project/project_reid.html">Market-1501</a>, Test on DukeMTMC-reID</li>
</ul>

<p>The primary motivation is that collecting ID annotation is relatively-expensive in human resource and time cost.</p>

<p>Is it possible to use less annotation on the unseen dataset, especially ID labels?</p>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Use DukeMTMC Training Data (without ID label but may use the camera ID)</th>
      <th>Rank@1</th>
      <th>mAP</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>UMDL</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>18.5%</td>
      <td>7.3%</td>
      <td>“<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S06-34.pdf">Unsupervised cross-dataset transfer learning for person re-identification</a>”, Peng Peixi, Tao Xiang, Yaowei Wang, Massimiliano Pontil, Shaogang Gong, Tiejun Huang, and Yonghong Tian, CVPR 2016</td>
    </tr>
    <tr>
      <td>Verif + Identif</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>25.7%</td>
      <td>12.8%</td>
      <td>“<a href="https://arxiv.org/abs/1611.05666">A Discriminatively Learned Cnn Embedding for Person Re-identification</a>”,  Zhedong Zheng, Liang Zheng, and Yi Yang, TOMM 2017. <a href="https://github.com/layumi/Person-reID-verification"><strong>[pytorch code]</strong></a>
</td>
    </tr>
    <tr>
      <td>PUL</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>30.4%</td>
      <td>16.8%</td>
      <td>“<a href="https://arxiv.org/abs/1705.10444">Unsupervised Person Re-identification: Clustering and Fine-tuning</a>”, Hehe Fan, Liang Zheng, Yi Yang, TOMM2018 <a href="https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning">[code]</a>
</td>
    </tr>
    <tr>
      <td>PN-GAN</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>29.9%</td>
      <td>15.8%</td>
      <td>“<a href="https://arxiv.org/pdf/1712.02225.pdf">Pose-Normalized Image Generation for Person Re-identification</a>” Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu, Yu-Gang Jiang, Xiangyang Xue, ECCV 2018</td>
    </tr>
    <tr>
      <td>OG-Net</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>31.3%</td>
      <td>16.3%</td>
      <td>“<a href="https://arxiv.org/abs/2006.04569">Parameter-Efficient Person Re-identification in the 3D Space</a>”,  Zhedong Zheng and Yi Yang, TNNLS 2022. <a href="https://github.com/layumi/person-reid-3d"><strong>[pytorch code]</strong></a>
</td>
    </tr>
    <tr>
      <td>SPGAN</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>41.4%</td>
      <td>22.3%</td>
      <td>“<a href="https://arxiv.org/abs/1711.07027">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</a>”, Weijian Deng, Liang Zheng, Guoliang Kang, Yi Yang, Qixiang Ye, Jianbin Jiao, CVPR 2018</td>
    </tr>
    <tr>
      <td>TJ-AIDL</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>44.3%</td>
      <td>23.0%</td>
      <td>“<a href="http://www.eecs.qmul.ac.uk/~xiatian/papers/WangEtAl_CVPR2018.pdf">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</a>”, Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li, ECCV 2018</td>
    </tr>
    <tr>
      <td>MMFA</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>45.3%</td>
      <td>24.7%</td>
      <td>“<a href="https://arxiv.org/abs/1807.01440">Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification</a>”, Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot, BMVC 2018</td>
    </tr>
    <tr>
      <td>DG-Net</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>43.5%</td>
      <td>25.4%</td>
      <td>“<a href="https://arxiv.org/abs/1904.07223">Joint Discriminative and Generative Learning for Person Re-identification</a>”,  Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang and Jan Kautz, CVPR 2019. (Results are in Appendix)</td>
    </tr>
    <tr>
      <td>SPGAN+LMP</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>46.4%</td>
      <td>26.2%</td>
      <td> </td>
    </tr>
    <tr>
      <td>HHL</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>46.9%</td>
      <td>27.2%</td>
      <td>“<a href="https://github.com/zhunzhong07/zhunzhong07.github.io/blob/master/paper/HHL.pdf">Generalizing A Person Retrieval Model Hetero- and Homogeneously</a>”, Zhun Zhong, Liang Zheng, Shaozi Li, Yi Yang, ECCV 2018</td>
    </tr>
    <tr>
      <td>BUC</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>47.4%</td>
      <td>27.5%</td>
      <td>“<a href="http://xuanyidong.com/pdf/AAAI19-vana.pdf">A Bottom-up Clustering Approach to Unsupervised Person Re-identification</a>”, Yutian Lin, Xuanyi Dong, Liang Zheng,Yan Yan, Yi Yang, AAAI 2018</td>
    </tr>
    <tr>
      <td>CFSM</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>49.8%</td>
      <td>27.3%</td>
      <td>“<a href="https://arxiv.org/abs/1812.02605">Disjoint Label Space Transfer Learning with Common Factorised Space</a>”, Xiaobin Chang, Yongxin Yang, Tao Xiang, Timothy M. Hospedales, AAAI 2019</td>
    </tr>
    <tr>
      <td>ARN</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>60.2%</td>
      <td>33.4%</td>
      <td>“<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w6/Li_Adaptation_and_Re-Identification_CVPR_2018_paper.pdf">Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</a>”, Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du, and Yu-Chiang Frank Wang, CVPR 2018 Workshop</td>
    </tr>
    <tr>
      <td>TAUDL</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>61.7%</td>
      <td>43.5%</td>
      <td>“<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Minxian_Li_Unsupervised_Person_Re-identification_ECCV_2018_paper.pdf">Unsupervised Person Re-identification by Deep Learning Tracklet Association</a>”, Minxian Li, Xiatian Zhu, and Shaogang Gong, ECCV 2018</td>
    </tr>
    <tr>
      <td>UDARTP</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>68.4%</td>
      <td>49.0%</td>
      <td>“<a href="https://arxiv.org/pdf/1807.11334.pdf">Unsupervised Domain Adaptive Re-Identification: Theory and Practice</a>”, Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, and Xinggang Wang, arXiv:1807.11334</td>
    </tr>
    <tr>
      <td>PCB-PAST</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>72.4%</td>
      <td>54.3%</td>
      <td>“<a href="https://arxiv.org/abs/1907.13315">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</a>” Xinyu Zhang, Jiewei Cao, Chunhua Shen, and Mingyu You. ICCV 2019</td>
    </tr>
    <tr>
      <td>SSG</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>73.0%</td>
      <td>53.4%</td>
      <td>“<a href="https://arxiv.org/abs/1811.10144">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</a>” Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, and Thomas S Huang. ICCV 2019</td>
    </tr>
    <tr>
      <td>MMCL</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>72.4%</td>
      <td>54.5%</td>
      <td>“<a href="https://arxiv.org/abs/2004.09228">Unsupervised Person Re-identification via Multi-label Classification</a>” Dongkai Wang and Shiliang Zhang. CVPR 2020.</td>
    </tr>
    <tr>
      <td>AD-Cluster</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>72.6%</td>
      <td>54.1%</td>
      <td>“<a href="https://arxiv.org/abs/2004.08787">AD-Cluster: Augmented Discriminative Clustering for Domain Adaptive Person Re-identification</a>” Yunpeng Zhai, Shijian Lu, Qixiang Ye, Xuebo Shan, Jie Chen, Rongrong Ji, and Yonghong Tian. CVPR 2020</td>
    </tr>
    <tr>
      <td>B-SNR+GDS-H</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>76.7%</td>
      <td>59.7%</td>
      <td>“<a href="https://arxiv.org/abs/2006.00752">Global Distance-distributions Separation for Unsupervised Person Re-identification</a>” Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen. ECCV 2020</td>
    </tr>
    <tr>
      <td>NRMT</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>77.8%</td>
      <td>62.2%</td>
      <td>“<a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560511.pdf">Unsupervised domain adaptation with noise resistible mutual-training for person re-identification</a>” Fang Zhao, Shengcai Liao, Guo-Sen Xie, Jian Zhao, Kaihao Zhang, and Ling Shao. ECCV 2020</td>
    </tr>
    <tr>
      <td>DAAM</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>77.6%</td>
      <td>63.9%</td>
      <td>“<a href="https://arxiv.org/abs/1905.10529">Domain Adaptive Attention Model for Unsupervised Cross-Domain Person Re-Identification</a>” Yangru Huang, Peixi Peng, Yi Jin, Junliang Xing, Congyan Lang, Songhe Feng. AAAI 2020</td>
    </tr>
    <tr>
      <td>MMT</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>78.0%</td>
      <td>65.1%</td>
      <td>“<a href="https://arxiv.org/abs/2001.01526">Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification</a>” Yixiao Ge, Dapeng Chen, Hongsheng Li. ICLR 2020</td>
    </tr>
    <tr>
      <td>DGNet++</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>78.9%</td>
      <td>63.8%</td>
      <td>“<a href="https://arxiv.org/abs/2007.10315">Joint disentangling and adaptation for cross-domain person re-identification</a>” Yang Zou, Xiaodong Yang, Zhiding Yu, B.V.K. Vijaya Kumar, Jan Kautz. ECCV20</td>
    </tr>
    <tr>
      <td>MEB-Net</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>79.6%</td>
      <td>66.1%</td>
      <td>“<a href="https://arxiv.org/abs/2007.01546">Multiple expert brainstorming for domain adaptive person re-identification</a>” Yunpeng Zhai, Qixiang Ye, Shijian Lu, Mengxi Jia, Rongrong Ji, Yonghong Tian. ECCV 2020</td>
    </tr>
    <tr>
      <td>UNRN</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>82.0%</td>
      <td>69.1%</td>
      <td>“<a href="https://arxiv.org/abs/2012.08733">Exploiting Sample Uncertainty for Domain Adaptive Person Re-Identification</a>” Kecheng Zheng, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, and Zheng-Jun Zha. AAAI 2021</td>
    </tr>
    <tr>
      <td>Cluster Contrast + GEM</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>86.8%</td>
      <td>76.0%</td>
      <td>“<a href="https://arxiv.org/abs/2103.11568">Cluster Contrast for Unsupervised Person Re-Identification</a>” Dai, Zuozhuo and Wang, Guangyuan and Zhu, Siyu and Yuan, Weihao and Tan, Ping. arXiv 2021</td>
    </tr>
  </tbody>
</table>

<h3 id="train-on-msmt17-test-on-dukemtmc-reid">Train on <a href="https://www.pkuvmc.com/publications/msmt17.html">MSMT17</a>, Test on DukeMTMC-reID</h3>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Use DukeMTMC Training Data (without ID label but may use the camera ID)</th>
      <th>Rank@1</th>
      <th>mAP</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Verif + Identif</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>48.7%</td>
      <td>27.5%</td>
      <td>“<a href="https://arxiv.org/abs/1611.05666">A Discriminatively Learned Cnn Embedding for Person Re-identification</a>”,  Zhedong Zheng, Liang Zheng, and Yi Yang, TOMM 2017. <a href="https://github.com/layumi/Person-reID-verification"><strong>[pytorch code]</strong></a>
</td>
    </tr>
    <tr>
      <td>OG-Net</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>31.3%</td>
      <td>16.3%</td>
      <td>“<a href="https://arxiv.org/abs/2006.04569">Parameter-Efficient Person Re-identification in the 3D Space</a>”,  Zhedong Zheng and Yi Yang, TNNLS 2022. <a href="https://github.com/layumi/person-reid-3d"><strong>[pytorch code]</strong></a>
</td>
    </tr>
    <tr>
      <td>DG-Net</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>62.0%</td>
      <td>40.7%</td>
      <td>“<a href="https://arxiv.org/abs/1904.07223">Joint Discriminative and Generative Learning for Person Re-identification</a>”,  Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang and Jan Kautz, CVPR 2019. <a href="https://github.com/NVlabs/DG-Net">[code]</a> (Results are in Appendix)</td>
    </tr>
    <tr>
      <td>MAR</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>67.1%</td>
      <td>48.0%</td>
      <td>“<a href="https://arxiv.org/abs/1903.06325">Unsupervised Person Re-identification by Soft Multilabel Learning</a>”, Hong-Xing Yu, Wei-Shi Zheng, Ancong Wu, Xiaowei Guo, Shaogang Gong, Jian-Huang Lai, CVPR 2019.</td>
    </tr>
    <tr>
      <td>UDARTP</td>
      <td><img class="emoji" title=":heavy_check_mark:" alt=":heavy_check_mark:" src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png" height="20" width="20"></td>
      <td>75.0%</td>
      <td>57.1%</td>
      <td>“<a href="https://arxiv.org/pdf/1807.11334.pdf">Unsupervised Domain Adaptive Re-Identification: Theory and Practice</a>”, Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, and Xinggang Wang, arXiv:1807.11334</td>
    </tr>
  </tbody>
</table>

<h3 id="train-on-msmt17-test-on-market">Train on <a href="https://www.pkuvmc.com/publications/msmt17.html">MSMT17</a>, Test on Market</h3>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Use Market Training Data (without ID label but may use the camera ID)</th>
      <th>Rank@1</th>
      <th>mAP</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OG-Net</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>40.1%</td>
      <td>17.6%</td>
      <td>“<a href="https://arxiv.org/abs/2006.04569">Parameter-Efficient Person Re-identification in the 3D Space</a>”,  Zhedong Zheng and Yi Yang, TNNLS 2022. <a href="https://github.com/layumi/person-reid-3d"><strong>[pytorch code]</strong></a>
</td>
    </tr>
    <tr>
      <td>DG-Net</td>
      <td><img class="emoji" title=":heavy_multiplication_x:" alt=":heavy_multiplication_x:" src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png" height="20" width="20"></td>
      <td>61.8%</td>
      <td>33.6%</td>
      <td>“<a href="https://arxiv.org/abs/1904.07223">Joint Discriminative and Generative Learning for Person Re-identification</a>”,  Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang and Jan Kautz, CVPR 2019. <a href="https://github.com/NVlabs/DG-Net">[code]</a> (Results are in Appendix)</td>
    </tr>
  </tbody>
</table>

<h2 id="dukemtmc-reid-protocol-citation">DukeMTMC-reID Protocol Citation</h2>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheng2017unlabeled</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zhedong and Zheng, Liang and Yang, Yi}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the IEEE International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2017}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ristani2016MTMC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ristani, Ergys and Solera, Francesco and Zou, Roger and Cucchiara, Rita and Tomasi, Carlo}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision workshop on Benchmarking Multi-Target Tracking}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span>
<span class="p">}</span>
</code></pre></div></div>


        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="https://github.com/layumi"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://zdzheng.xyz/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2024 Zhedong Zheng. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <!--
<script src="https://zdzheng.xyz/assets/js/main.min.js" async></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>




-->


  </body>
</html>

