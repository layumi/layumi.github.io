

<!doctype html>
<html lang="en" class="no-js">
  <head>
	<meta name="viewport" content="width=device-width, initial-scale=1">
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>UAVM 2023 - Zhedong Zheng</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Zhedong Zheng">
<meta property="og:title" content="UAVM 2023">








  <link rel="canonical" href="https://zdzheng.xyz/ACMMM2023Workshop">
  <meta property="og:url" content="https://zdzheng.xyz/ACMMM2023Workshop">























<!-- end SEO -->


<link href="https://zdzheng.xyz/feed.xml" type="application/atom+xml" rel="alternate" title="Zhedong Zheng Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<!--
<meta name="viewport" content="width=device-width, initial-scale=1.0">
-->
<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/main.css">

<!-- favicon -->
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">

<meta http-equiv="cleartype" content="on">

    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://zdzheng.xyz/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://zdzheng.xyz/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://zdzheng.xyz/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://zdzheng.xyz/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://zdzheng.xyz/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://zdzheng.xyz/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://zdzheng.xyz/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://zdzheng.xyz/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://zdzheng.xyz/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="mask-icon" href="https://zdzheng.xyz/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://zdzheng.xyz/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://zdzheng.xyz/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://zdzheng.xyz/">Zhedong Zheng</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/">Home</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/research">Research</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/files/zhedong-resume.pdf">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/seminar/">Seminar</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/recruitment/">Join Us</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<head> 
	<style>
		.author {
			text-decoration: none !important;
			color: #333333;
		}

		.author:hover {
			text-decoration: underline;
			color: #0066cc;
		}
	    pre {
	      background-color: gray;
	      color: white;
	      padding: 10px;
	      border-radius: 5px;
	    }
	</style>
</head>

<div id="main" role="main">
  <meta name="og:description" content="UAVM 2023"> 
  



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="UAVM 2023">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">UAVM 2023
</h1>
          
        
        
        
        

        
		
		
		
		
			
		
		  <meta name="description" content="UAVM 2023">
		
		
		
		
		
		
		
		
		
    
        </header>
      

      <section class="page__content" itemprop="text">
        <div align="center"> 
  <h2> ACM Multimedia </h2>
 </div>

<div align="center" style="vertical-align:middle"> 
  <h2> <img src="https://www.acmmm2023.org/wp-content/uploads/2022/12/cropped-cropped-sigmmlogo-1.gif" margn-right="20px" /><a href="https://www.acmmm2023.org/"> ACM MM 2023 </a><a href="https://www.acmmm2023.org/">(https://www.acmmm2023.org/)</a>  </h2>
 </div>

<div align="center"> 
  <h2> Workshop on </h2>
  <h2> UAVs in Multimedia: Capturing the World from a New Perspective (UAVM 2023)
</h2>
 <img src="https://github.com/layumi/ACMMM2023Workshop/blob/main/picture/logo.png?raw=true" />
</div>

<meta name="og:image" content="https://github.com/layumi/ACMMM2023Workshop/blob/main/picture/logo.png?raw=true" />

<meta name="og:description" content="UAVs in Multimedia: Capturing the World from a New Perspective (UAVM 2023)" />

<p>The accept papers will be published at ACM Multimedia Workshop (top 50%), and go through the same peer review process as the regular papers. Several authors will be invited to do a oral presentation.</p>

<p><a href="https://zdzheng.xyz/files/ACMMM23_Workshop_Drone.pdf">[Accepted Workshop Proposal]</a>
<a href="https://openreview.net/group?id=acmmm.org/ACMMM/2023/Workshop/UAVs_in_Multimedia">[Submission Site]</a></p>

<p>Join our <a href="https://groups.google.com/g/acmmm-uvas-2023-participants">Google Group</a> for important updates.</p>

<h2 id="news">News</h2>
<ul>
  <li>28/10/2023 - All proceeding papers can be found at <a href="https://dl.acm.org/doi/proceedings/10.1145/3607834">ACM Website</a></li>
  <li>15/7/2023 - Challenge Open-source Code.</li>
  <li>23/4/2023 - Challenge Platform is now available.</li>
  <li>7/4/2023 - Paper submission site is now available.</li>
  <li>6/4/2023 - CFP is released.</li>
  <li>6/4/2023 - Workshop homepage is now available.</li>
</ul>

<h2 id="workshop-schedule">Workshop Schedule</h2>

<p>2 Nov 9:30~10:30am (GMT-4)  Nathan Jacobs (Washington University in St. Louis)  (Last 10 minutes will be QA)</p>

<p>The talk slides can be found at <a href="https://www.zdzheng.xyz/files/Talk-Nathan-Jacobs.pdf">[link]</a>.</p>

<p>2 Nov 10:30~11:30am (GMT-4) Rakesh Kumar (SRI International)  (Last 10 minutes will be QA)</p>

<p>The talk slides can be found at <a href="https://www.zdzheng.xyz/files/Kumar%20Keynote%20ACM%20MM%20UAV%20MM%20workshop%202023.pdf">[link]</a>.</p>

<p>2 Nov 11:30~11:45am break</p>

<p>2 Nov 11:45~12:00am (GMT-4) Challenge 1st-place Winner</p>

<p>2 Nov 12:00~12:15am (GMT-4) Challenge 2nd-place Winner</p>

<p>2 Nov 12:15~12:30am (GMT-4) Challenge 3rd-place Winner</p>

<h2 id="invited-speakers">Invited Speakers</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://jacobsn.github.io/images/nathan_jacobs.jpg" width="160" /></th>
      <th style="text-align: center"><img src="https://www.sri.com/wp-content/uploads/2021/10/Kumar-Web-Portrait-191126-400x400-1-e1636493948651.jpg" width="160" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://jacobsn.github.io/">Nathan Jacobs</a>, Washington University in St. Louis</td>
      <td style="text-align: center"><a href="https://www.sri.com/about/people/rakesh-teddy-kumar">Rakesh Kumar</a>, SRI International</td>
    </tr>
  </tbody>
</table>

<p><strong>Talk: Learning to Map Anything, Anywhere, Anytime</strong> (<a href="https://jacobsn.github.io/">Nathan Jacobs</a>)</p>

<p>Abstract: What might it sound like here? How would you describe this place? Would it be unusual to see a large mammal if I took an early morning walk? These are all questions that are inherently spatial in nature and difficult to answer precisely. This talk explores a new approach to multi-modal remote sensing that shows how we might build a system that supports answering such questions at a global scale, enabling us to understand the Earth with a level of semantic, spatial, and temporal resolution that was previously impossible.</p>

<p>Bio：Nathan Jacobs earned a Ph.D. in Computer Science at Washington University in St. Louis (2010). After many years at the University of Kentucky, he is currently a Professor in the Computer Science &amp; Engineering department at Washington University in St. Louis. Dr. Jacobs’ research area is computer vision; his specialty is developing learning-based algorithms and systems for processing large-scale image collections. His current focus is on developing techniques for mining information about people and the natural world from geotagged imagery, including images from social networks, publicly available outdoor webcams, and satellites. His research has been funded by NSF, NIH, DARPA, IARPA, NGA, ARL, AFRL, and Google.</p>

<p><strong>Talk：Semantically Guided Collaborative Navigation, 3D Mapping, Planning and Control for Unmanned Platforms</strong> (<a href="https://www.sri.com/about/people/rakesh-teddy-kumar">Rakesh Kumar</a>)</p>

<p>Abstract：Classical metric navigation systems use GPS and/ or prior 2D and 3D maps for localization of unmanned platforms. However, these systems do not operate n in GPS challenged or denied environments. Metric map-based navigation systems are also not robust to dynamic scene changes. In this talk, we will describe various methods we have developed to incorporate AI derived semantic information into metric navigation for both ground and aerial robots using data from multiple sensors. Compared to low-level metric features, semantic information is more robust to scene changes over time and can be matched across time/space/platforms. We will also discuss how 3D reference semantic maps can be built.
Sharing semantic information also reduces bandwidth required for collaboration. Moreover, it enables natural language interaction between humans and mobile platforms. Finally, we will discuss how the semantic information can be used by robots to learn to navigate in unmapped areas much like humans are able to visit and navigate in new, never visited before locales. Our new approach, SayNav, leverages common-sense knowledge from Large Language Models (LLMs) for efficient generalization to complicated navigation tasks in unknown large-scale environments.</p>

<p>Bio：Rakesh “Teddy” Kumar, Ph.D., is Vice President, Information and Computing Sciences and Director of the Center for Vision Technologies at SRI International. In this role, he is responsible for leading research and development of innovative end-to-end vision solutions from image capture to situational understanding that translate into real-world applications such as robotics, intelligence extraction and human computer interaction. He has received the Outstanding Achievement in Technology Development award from his alma mater, University of Massachusetts Amherst, the Sarnoff Presidents Award, and Sarnoff Technical Achievement awards for his work in registration of multi-sensor, multi-dimensional medical images and alignment of video to three-dimensional scene models. The paper “Stable Vision-Aided Navigation for Large-Area Augmented Reality” co-authored by him received the best paper award in the IEEE Virtual Reality 2011 conference. The paper “Augmented Reality Binoculars” co-authored by him received the best paper award in the IEEE International Symposium on Mixed and Augmented Reality (ISMAR) 2013 conference. 
Kumar has served on NSF review and DARPA ISAT panels. He has also been an associate editor for IEEE Transactions on Pattern Analysis and Machine Intelligence. He has co-authored more than 60 research publications, and received more than 50 patents. A number of spin-off companies have been created based on the research done at the Center for Vision Technologies. Kumar received his Ph.D. in Computer Science from the University of Massachusetts at Amherst in 1992. His M.S. in Electrical and Computer Engineering is from State University of New York at Buffalo in 1995, and his B.Tech in Electrical Engineering is from Indian Institute of Technology, Kanpur, India in 1983.</p>

<h2 id="winners">Winners</h2>
<ol>
  <li>
    <p><strong>Fabian Deuser<sup>1</sup>, Konrad Habel1, Martin Werner<sup>2</sup>, Norbert Oswald<sup>1</sup></strong> (<sup>1</sup>University of the Bundeswehr Munich, <sup>2</sup>Technische Universität München)</p>
  </li>
  <li>
    <p><strong>Zhifeng Lin<sup>1,2</sup>, Ranran Huang<sup>1</sup>, Jiancheng Cai<sup>1</sup>, Xinmin Liu<sup>1</sup>, Changxing Ding<sup>2</sup>, Zhenhua Chai<sup>1</sup></strong> (<sup>1</sup>Meituan, <sup>2</sup>South China University of Technology)</p>
  </li>
  <li>
    <p><strong>Haoran Li, Quan Chen, Zhiwen Yang, Jiong Yin</strong> (Hangzhou Dianzi University)</p>
  </li>
</ol>

<h2 id="challenge-open-source-codes">Challenge Open-source Codes</h2>
<p>Please check <a href="https://github.com/layumi/UAVM2023">https://github.com/layumi/UAVM2023</a></p>

<h2 id="important-dates">Important Dates</h2>

<p><strong>Submission of papers:</strong></p>
<ul>
  <li>Workshop Papers Submission: <del>5 July 2023</del> 13 July 2023</li>
  <li>Workshop Papers Notification: 30 July 2023</li>
  <li>Student Travel Grants Application Deadline: 5 August 2023</li>
  <li>Camera-ready Submission: 6 August 2023</li>
  <li>Conference Dates: 28 October 2023 – 3 November 2023</li>
</ul>

<p>Please note: The submission deadline is at 11:59 p.m. of the stated deadline date <a href="https://time.is/Anywhere_on_Earth">Anywhere on Earth</a></p>

<h2 id="abstract">Abstract</h2>
<p>Unmanned Aerial Vehicles (UAVs), also known as drones, have become increasingly popular in recent years due to their ability to capture high-quality multimedia data from the sky. With the rise of multimedia applications, such as aerial photography, cinematography, and mapping, UAVs have emerged as a powerful tool for gathering rich and diverse multimedia content. This workshop aims to bring together researchers, practitioners, and enthusiasts interested in UAV multimedia to explore the latest advancements, challenges, and opportunities in this exciting field. The workshop will cover various topics related to UAV multimedia, including aerial image and video processing, machine learning for UAV data analysis, UAV swarm technology, and UAV-based multimedia applications. In the context of the ACM Multimedia conference, this workshop is highly relevant as multimedia data from UAVs is becoming an increasingly important source of content for many multimedia applications. The workshop will provide a platform for researchers to share their work and discuss potential collaborations, as well as an opportunity for practitioners to learn about the latest developments in UAV multimedia technology.
Overall, this workshop will provide a unique opportunity to explore the exciting and rapidly evolving field of UAV multimedia and its potential impact on the wider multimedia community.</p>

<p><strong>The list of possible topics includes, but is not limited to:</strong></p>

<ul>
  <li>Video-based UAV Navigation
    <ul>
      <li>Satellite-guided &amp; Ground-guided Navigation</li>
      <li>Path Planning and Obstacle Avoidance</li>
      <li>Visual SLAM (Simultaneous Localization and Mapping)</li>
      <li>Sensor Fusion and Reinforcement Learning for Navigation</li>
    </ul>
  </li>
  <li>UAV Swarm Coordination
    <ul>
      <li>Multiple Platform Collaboration</li>
      <li>Multi-agent Cooperation and Communication</li>
      <li>Decentralized Control and Optimization</li>
      <li>Distributed Perception and Mapping</li>
    </ul>
  </li>
  <li>UAV-based Object Detection and Tracking
    <ul>
      <li>Aerial-view Object Detection, Tracking and Re-identification</li>
      <li>Aerial-view Action Recognition</li>
    </ul>
  </li>
  <li>UAV-based Sensing and Mapping
    <ul>
      <li>3D Mapping and Reconstruction</li>
      <li>Remote Sensing and Image Analysis</li>
      <li>Disaster Response and Relief</li>
    </ul>
  </li>
  <li>UAV-based Delivery and Transportation
    <ul>
      <li>Package Delivery and Logistics</li>
      <li>Safety and Regulations for UAV-based Transportation</li>
    </ul>
  </li>
</ul>

<h2 id="submission-types">Submission Types</h2>

<p>Paper can be submitted on <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2023/Workshop/UAVs_in_Multimedia">[Open Review]</a>.</p>

<p>Submission template can be found at <a href="https://www.acm.org/publications/proceedings-template">ACM</a> or you may directly follow the <a href="https://www.overleaf.com/read/yfpxtyngmzjn">overleaf template</a>.</p>

<p>In this workshop, we welcome four types of submissions, all of which should relate to the topics and themes as listed in Section 3:</p>

<ul>
  <li>(1). Position or perspective papers (<strong>up to 4 pages in length, plus unlimited pages for references</strong>): original ideas, perspectives, research vision, and open challenges in the area of evaluation approaches for explainable recommender systems;</li>
  <li>(2). Challenge papers (<strong>up to 4 pages in length, plus unlimited pages for references</strong>): original solution to the Challenge data, University160k, in terms of effectiveness and efficiency.</li>
  <li>(3). Featured papers (title and abstract of the paper, plus the original paper): already published papers or papers summarizing existing publications in leading conferences and highimpact journals that are relevant for the topic of the workshop;</li>
  <li>(4). Demonstration papers (<strong>up to 2 pages in length, plus unlimited pages for references</strong>): original or already published prototypes and operational evaluation approaches in the area of explainable recommender systems. Page limits include diagrams and appendices. Submissions should be single-blind, written in English, and formatted according to the current ACM two-column conference format. Suitable LaTeX, Word, and Overleaf templates are available from the ACM Website (use “sigconf” proceedings template for LaTeX and the Interim Template for Word).</li>
</ul>

<p><strong>Tips:</strong></p>
<ul>
  <li>For privacy protection, please blur faces in the published materials (such as paper, video, poster, etc.)</li>
  <li>For social good, please do not contain any misleading words, such as <code class="language-plaintext highlighter-rouge">surveillance</code> and  <code class="language-plaintext highlighter-rouge">secret</code>.</li>
</ul>

<h2 id="challenge">Challenge</h2>

<p>Challenge Platform is at https://codalab.lisn.upsaclay.fr/competitions/12672 .</p>

<p>We also provide a challenging cross-view geo-localization dataset, called University160k, and the workshop audience may consider to participate the competition. The motivation is to simulate the real- world geo-localization scenario that we usually face an extremely large satellite-view pool. In particular, University160k extends the current University-1652 dataset with extra 167,486 satellite- view gallery distractors. We will release University160k on our website, and make a public leader board. These distractor satellite- view images have a size of 1024 × 1024 and are obtained by cutting orthophoto images of real urban and surrounding areas. The larger image size ensures higher image clarity, while the wider framing range allows the images to contain more diverse scenes, such as buildings, city roads, trees, fields, and more (see Figure 3). In our primary evaluation, the distractor is challenging and make the competitive baseline model, LPN, decrease the Recall@1 accuracy from 75.93% to 64.85% and the value of AP from 79.14% to 67.69% in the Drone → Satellite task (Please see Table 2). We hope more audiences can be involved to solve this challenge, and may also consider the efficiency problem against a large candidate pool.</p>

<p>Check challenge details at Section 5 in <a href="https://zdzheng.xyz/files/ACMMM23_Workshop_Drone.pdf">https://zdzheng.xyz/files/ACMMM23_Workshop_Drone.pdf</a></p>

<p>The challenge dataset contains two part.</p>
<ol>
  <li>
    <p>The basic dataset (training set) can be download by <a href="https://github.com/layumi/University1652-Baseline/blob/master/Request.md">Request</a>. Usually I will reply the download link in 5 minutes.</p>
  </li>
  <li>
    <p>The name-masked test-160k dataset (query &amp; gallery+distractor) can be downloaded from <a href="https://hdueducn-my.sharepoint.com/:f:/g/personal/wongtyu_hdu_edu_cn/EhWkk5hLBfBPpknm0sSPDiAB_SJZdJaXqTU1zRp19APDYQ?e=ummz0M">Onedrive</a>.</p>
  </li>
</ol>

<p>(In the future, you also can download the name-unmasked distractor dataset to quiclyt report number in your paper (Please add to satellite gallery) can be downloaded from <a href="https://hdueducn-my.sharepoint.com/:u:/g/personal/wongtyu_hdu_edu_cn/EY4gu3JwWO9IkAp707N7wT0BkQOXFi-ZSDGnWkG9bad1_A?e=hodpkb">Onedrive</a>, <a href="https://drive.google.com/file/d/1kYUBJazF0gqs2UQD46PvvJgnMVlJMPEs/view">Google Drive</a>,
or Baidu Disk(https://pan.baidu.com/s/15TDqJIkEVv2r1fWlLQFLPw Code:78xf).)</p>

<p>The submission example can be found at <a href="https://github.com/layumi/ACMMM2023Workshop/blob/main/answer.txt">Baseline Submission</a>. Please zip it as ``answer.zip’’ to submit the result.</p>

<p>Please return the top-10 satellite names. For example, the first query is <code class="language-plaintext highlighter-rouge">Y2HVQvCQIwVmwzq.jpeg''. Therefore, the first line of returned result in</code>answer.txt’’ should be the format as follows:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LJMJGM5vTQM3iRy	ValP4k9neTZffLz	Co1CEWkBhHdTAM2	w2Nk6LrN5p2cF54	FuMp6XdwlRqScG2	4WVhVPBkr8TJTNJ	y7XiwY8lWpMZNar	AQZgRYUIyvpUnz8	bziEPp56rwI7e7E	qI9WAxrCnbaqjIq
</code></pre></div></div>
<p>Please return the result following the order of query at <a href="https://github.com/layumi/ACMMM2023Workshop/blob/main/query_drone_name.txt">Query TXT</a>
It will be 37855 lines.</p>

<h2 id="organizing-team">Organizing Team</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://github.com/layumi/ICME2022SS/blob/main/picture/1.png?raw=true" width="160" /></th>
      <th style="text-align: center"><img src="https://yujiaoshi.github.io/images/YujiaoShiCircle.jpg" width="160" /></th>
      <th style="text-align: center"><img src="https://github.com/wtyhub/Photo/blob/a713229943f0628ffb82556bc9e396bbfabe8567/1%20inch.jpg?raw=true" width="160" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://zdzheng.xyz">Zhedong Zheng</a>, National University of Singapore, Singapore</td>
      <td style="text-align: center"><a href="https://yujiaoshi.github.io/">Yujiao Shi</a>, Australian National University, Australia</td>
      <td style="text-align: center"><a href="https://scholar.google.com/citations?user=wv3H-F4AAAAJ">Tingyu Wang</a>, Hangzhou Dianzi University, China</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="https://istd.sutd.edu.sg/files/xistd-faculty-liu-jun-2021.jpg.pagespeed.ic.kj4jHLG_to.webp" width="160" /></td>
      <td style="text-align: center"><img src="https://jwfangit.github.io/img/pic.jpg" width="160" /></td>
      <td style="text-align: center"><img src="https://weiyc.github.io/images/people/wyc.jpg" width="160" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://istd.sutd.edu.sg/people/faculty/liu-jun">Jun Liu</a>, Singapore University of Technology and Design, Singapore</td>
      <td style="text-align: center"><a href="https://jwfangit.github.io/">Jianwu Fang</a>, Chang’an University, China</td>
      <td style="text-align: center"><a href="https://weiyc.github.io">Yunchao Wei</a>, Beijing Jiaotong University, China</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="https://github.com/layumi/ACMMM2023Workshop/blob/main/picture/5.png?raw=true" width="160" /></td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://www.chuatatseng.com">Tat-Seng Chua</a>, National University of Singapore, Singapore</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<h2 id="conference-and-journal-papers">Conference and Journal Papers</h2>

<p>All papers presented at ACMMM 2023 will be included in ACM proceeding. All papers submitted to this workshop will go through the same review process as the regular papers submitted to the main conference to ensure that the contributions are of high quality.</p>

<h2 id="student-traval-funding">Student Traval Funding</h2>

<p>Please check https://www.acmmm2023.org/student-travel-grants/</p>

<p>Application Deadline: August 5, 2023</p>

<h2 id="workshop-citation">Workshop Citation</h2>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheng2023UVA</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{UAVM '23: 2023 Workshop on UAVs in Multimedia: Capturing the World from a New Perspective}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zhedong and Shi, Yujiao and Wang, Tingyu and Liu, Jun and Fang, Jianwu and Wei, Yunchao and Chua, Tat-seng}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the 31th ACM International Conference on Multimedia Workshop}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>  
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="https://github.com/layumi"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://zdzheng.xyz/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Zhedong Zheng. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <!--
<script src="https://zdzheng.xyz/assets/js/main.min.js" async></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>




-->


  </body>
</html>

