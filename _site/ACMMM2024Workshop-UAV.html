

<!doctype html>
<html lang="en" class="no-js">
  <head>
	<meta name="viewport" content="width=device-width, initial-scale=1">
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>UAVM 2024 - Zhedong Zheng</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Zhedong Zheng">
<meta property="og:title" content="UAVM 2024">








  <link rel="canonical" href="https://zdzheng.xyz/ACMMM2024Workshop-UAV">
  <meta property="og:url" content="https://zdzheng.xyz/ACMMM2024Workshop-UAV">























<!-- end SEO -->


<link href="https://zdzheng.xyz/feed.xml" type="application/atom+xml" rel="alternate" title="Zhedong Zheng Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<!--
<meta name="viewport" content="width=device-width, initial-scale=1.0">
-->
<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/main.css">

<!-- favicon -->
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">

<meta http-equiv="cleartype" content="on">

    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://zdzheng.xyz/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://zdzheng.xyz/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://zdzheng.xyz/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://zdzheng.xyz/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://zdzheng.xyz/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://zdzheng.xyz/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://zdzheng.xyz/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://zdzheng.xyz/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://zdzheng.xyz/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="mask-icon" href="https://zdzheng.xyz/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://zdzheng.xyz/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://zdzheng.xyz/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://zdzheng.xyz/">Zhedong Zheng</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/">Home</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/research">Research</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/files/zhedong-resume.pdf">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/seminar/">Seminar</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/recruitment/">Join Us</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<head> 
	<style>
		.author {
			text-decoration: none !important;
			color: #333333;
		}

		.author:hover {
			text-decoration: underline;
			color: #0066cc;
		}
	    pre {
	      background-color: gray;
	      color: white;
	      padding: 10px;
	      border-radius: 5px;
	    }
	</style>
</head>

<div id="main" role="main">
  <meta name="og:description" content="UAVM 2024"> 
  



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="UAVM 2024">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">UAVM 2024
</h1>
          
        
        
        
        

        
		
		
		
		
			
		
		  <meta name="description" content="UAVM 2024">
		
		
		
		
		
		
		
		
		
    
        </header>
      

      <section class="page__content" itemprop="text">
        <div align="center"> 
  <h2> ACM Multimedia </h2>
 </div>

<div align="center" style="vertical-align:middle"> 
  <h2> <img src="https://2024.acmmm.org/img/mm_logo_nobg.9dc2d630.png" margn-right="20px" /><a href="https://2024.acmmm.org/"> ACM MM 2024 </a><a href="https://2024.acmmm.org/">(https://2024.acmmm.org/)</a>  </h2>
 </div>

<div align="center"> 
  <h2> Workshop on </h2>
  <h2> UAVs in Multimedia: Capturing the World from a New Perspective (UAVM 2024)
</h2>
 <img src="https://github.com/layumi/ACMMM2023Workshop/blob/main/picture/logo.png?raw=true" />
</div>

<meta name="og:image" content="https://github.com/layumi/ACMMM2023Workshop/blob/main/picture/logo.png?raw=true" />

<meta name="og:description" content="UAVs in Multimedia: Capturing the World from a New Perspective (UAVM 2024)" />

<p>The accept papers will be published at ACM Multimedia Workshop (top 50%), and go through the same peer review process as the regular papers. Several authors will be invited to do a oral presentation.</p>

<p><a href="https://zdzheng.xyz/files/MM24_Workshop_Proposal_Drone.pdf">[Accepted Workshop Proposal]</a>
<a href="https://openreview.net/group?id=acmmm.org/ACMMM/2024/Workshop/UAVM">[Submission Site]</a></p>

<h2 id="news">News</h2>
<ul>
  <li>25/7/2024 - Challenge Open-source Code.</li>
  <li>23/4/2024 - Challenge Platform is now available.</li>
  <li>23/4/2024 - Paper submission site is now available.</li>
  <li>22/4/2024 - CFP is released.</li>
  <li>22/4/2024 - Workshop homepage is now available.</li>
</ul>

<h2 id="workshop-schedule">Workshop Schedule</h2>

<p>11:30~11:45am break</p>

<p>11:45~12:00am (GMT-4) Challenge 1st-place Winner</p>

<p>12:00~12:15am (GMT-4) Challenge 2nd-place Winner</p>

<p>12:15~12:30am (GMT-4) Challenge 3rd-place Winner</p>

<h2 id="important-dates">Important Dates</h2>

<p><strong>Submission of papers:</strong></p>
<ul>
  <li>Workshop Papers Submission: 5 July 2024</li>
  <li>Workshop Papers Notification: 30 July 2024</li>
  <li>Student Travel Grants Application Deadline: 5 August 2024</li>
  <li>Camera-ready Submission: 6 August 2024</li>
  <li>Conference Dates: 28 October 2024 – 1 November 2024</li>
</ul>

<p>Please note: The submission deadline is at 11:59 p.m. of the stated deadline date <a href="https://time.is/Anywhere_on_Earth">Anywhere on Earth</a></p>

<h2 id="abstract">Abstract</h2>
<p>Unmanned Aerial Vehicles (UAVs), also known as drones, have become increasingly popular in recent years due to their ability to capture high-quality multimedia data from the sky. With the rise of multimedia applications, such as aerial photography, cinematography, and mapping, UAVs have emerged as a powerful tool for gathering rich and diverse multimedia content. This workshop aims to bring together researchers, practitioners, and enthusiasts interested in UAV multimedia to explore the latest advancements, challenges, and opportunities in this exciting field. The workshop will cover various topics related to UAV multimedia, including aerial image and video processing, machine learning for UAV data analysis, UAV swarm technology, and UAV-based multimedia applications. In the context of the ACM Multimedia conference, this workshop is highly relevant as multimedia data from UAVs is becoming an increasingly important source of content for many multimedia applications. The workshop will provide a platform for researchers to share their work and discuss potential collaborations, as well as an opportunity for practitioners to learn about the latest developments in UAV multimedia technology.
Overall, this workshop will provide a unique opportunity to explore the exciting and rapidly evolving field of UAV multimedia and its potential impact on the wider multimedia community.</p>

<p><strong>The list of possible topics includes, but is not limited to:</strong></p>

<ul>
  <li>Video-based UAV Navigation
    <ul>
      <li>Satellite-guided &amp; Ground-guided Navigation</li>
      <li>Path Planning and Obstacle Avoidance</li>
      <li>Visual SLAM (Simultaneous Localization and Mapping)</li>
      <li>Sensor Fusion and Reinforcement Learning for Navigation</li>
    </ul>
  </li>
  <li>UAV Swarm Coordination
    <ul>
      <li>Multiple Platform Collaboration</li>
      <li>Multi-agent Cooperation and Communication</li>
      <li>Decentralized Control and Optimization</li>
      <li>Distributed Perception and Mapping</li>
    </ul>
  </li>
  <li>UAV-based Object Detection and Tracking
    <ul>
      <li>Aerial-view Object Detection, Tracking and Re-identification</li>
      <li>Aerial-view Action Recognition</li>
    </ul>
  </li>
  <li>UAV-based Sensing and Mapping
    <ul>
      <li>3D Mapping and Reconstruction</li>
      <li>Remote Sensing and Image Analysis</li>
      <li>Disaster Response and Relief</li>
    </ul>
  </li>
  <li>UAV-based Delivery and Transportation
    <ul>
      <li>Package Delivery and Logistics</li>
      <li>Safety and Regulations for UAV-based Transportation</li>
    </ul>
  </li>
</ul>

<h2 id="submission-types">Submission Types</h2>

<p>Paper can be submitted on <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2024/Workshop/UAVM">[Open Review]</a>.</p>

<p>Submission template can be found at <a href="https://www.acm.org/publications/proceedings-template">ACM</a> or you may directly follow the <a href="https://www.overleaf.com/read/yfpxtyngmzjn">overleaf template</a>.</p>

<p>In this workshop, we welcome four types of submissions, all of which should relate to the topics and themes as listed in Section 3:</p>

<ul>
  <li>(1). Position or perspective papers (<strong>up to 4 pages in length, plus 2 pages for references</strong>): original ideas, perspectives, research vision, and open challenges in the area of evaluation approaches for explainable recommender systems;</li>
  <li>(2). Challenge papers (<strong>up to 4 pages in length, plus 2 pages for references</strong>): original solution to the Challenge data, University160k, in terms of effectiveness and efficiency.</li>
  <li>(3). Demonstration papers (<strong>up to 2 pages in length, plus 2 pages for references</strong>): original or already published prototypes and operational evaluation approaches in the area of explainable recommender systems. Page limits include diagrams and appendices. Submissions should be single-blind, written in English, and formatted according to the current ACM two-column conference format. Suitable LaTeX, Word, and Overleaf templates are available from the ACM Website (use “sigconf” proceedings template for LaTeX and the Interim Template for Word).</li>
</ul>

<p><strong>Tips:</strong></p>
<ul>
  <li>For privacy protection, please blur faces in the published materials (such as paper, video, poster, etc.)</li>
  <li>For social good, please do not contain any misleading words, such as <code class="language-plaintext highlighter-rouge">surveillance</code> and  <code class="language-plaintext highlighter-rouge">secret</code>.</li>
</ul>

<h2 id="challenge">Challenge</h2>

<p><strong>Challenge Platform is at https://codalab.lisn.upsaclay.fr/competitions/18770 .</strong></p>

<p>We also provide a multi-weather cross-view geo-localization dataset, called University160k-WX, and welcome your participation in the competition. The motivation is to simulate the real-world geo-localization scenario. 
In particular, University160k extends the current University-1652 dataset with extra 167,486 satellite-view gallery distractors. 
University160k-WX further introduces weather variants on University160k, including fog, rain, snow and multiple weather compositions.
We will release University160k-WX on our website, and make a public leader board. 
These distractor satellite-view images have a size of $1024 \times 1024$ and are obtained by cutting orthophoto images of real urban and surrounding areas.
Multiple weathers are randomly sampled to increase the difficulty of representation learning. 
In our primary evaluation, the distractor is challenging and makes the competitive baseline model, LPN, decrease the Recall@1 accuracy from $75.93\%$ to $64.85\%$ and the value of AP from $79.14\%$ to $67.69\%$ in the Drone $\rightarrow$ Satellite task. If we further introduce extreme weather, the performance further drops from $64.85\%$ to $7.94\%$. 
We hope more audiences can be involved to solve this challenge, and consider the robustness problem against extreme weather.</p>

<p>Check challenge details at Section 5 in <a href="https://zdzheng.xyz/files/MM24_Workshop_Proposal_Drone.pdf">https://zdzheng.xyz/files/MM24_Workshop_Proposal_Drone.pdf</a></p>

<p>The challenge dataset contains two part.</p>
<ol>
  <li>
    <p>The basic dataset (training set) can be download by <a href="https://github.com/layumi/University1652-Baseline/blob/master/Request.md">Request</a>. Usually I will reply the download link in 5 minutes.</p>
  </li>
  <li>
    <p>The name-masked test-160k-WX dataset (query &amp; gallery+distractor) can be downloaded from <a href="https://hdueducn-my.sharepoint.com/:f:/g/personal/wongtyu_hdu_edu_cn/EoP7u9rymIxMrv_s3Im-vvQBX8PQ_4v1xzXolMfyKmINkw?e=N8vpum">Onedrive</a>.
Since only drone will meet weather conditions, we only simulate weather on drone-view queries.</p>
  </li>
</ol>

<p>The submission example can be found at <a href="https://github.com/wtyhub/ACMMM2024Workshop/blob/main/answer.txt">Baseline Submission</a>. Please zip it as <code class="language-plaintext highlighter-rouge">answer.zip</code> to submit the result.</p>

<p>Please return the top-10 satellite names. For example, the first query is <code class="language-plaintext highlighter-rouge">Q3JI2tUwDkhcfip.jpeg</code>. Therefore, the first line of returned result in <code class="language-plaintext highlighter-rouge">answer.txt</code> should be the format as follows:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>e6kXgz36E8nOY2n       ioqKwvSIYYhiW2v       y4VmQPUYOMD8AH4       kpZ2QJlNBHMnbRA       xffJQs2n9DP17fg       IejrFHLQYBfce2y       cH79t5WJMEMZ3VA       W9u0j4N1nlFbI97       zDurtAW4FTJfNJ3       MuvIMNVdofmaRqG
</code></pre></div></div>
<p>Please return the result following the order of query at <a href="https://github.com/wtyhub/ACMMM2024Workshop/blob/main/query_drone_name.txt">Query TXT</a>
It will be 37855 lines.</p>

<h2 id="related-papers">Related Papers</h2>
<ul>
  <li>Wang, T., Zheng, Z., Sun, Y., Yan, C., Yang, Y., &amp; Chua, T. S. (2024). Multiple-environment Self-adaptive Network for Aerial-view Geo-localization. Pattern Recognition, 152, 110363.</li>
  <li>Zheng, Z., Wei, Y., &amp; Yang, Y. (2020, October). University-1652: A multi-view multi-source benchmark for drone-based geo-localization. In Proceedings of the 28th ACM international conference on Multimedia (pp. 1395-1403).</li>
  <li>Wang, C., Zheng, Z., Quan, R., Sun, Y., &amp; Yang, Y. (2023). Context-aware pretraining for efficient blind image decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18186-18195).</li>
</ul>

<h2 id="organizing-team">Organizing Team</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="https://github.com/layumi/ICME2022SS/blob/main/picture/1.png?raw=true" width="160" /></th>
      <th style="text-align: center"><img src="https://yujiaoshi.github.io/images/YujiaoShiCircle.jpg" width="160" /></th>
      <th style="text-align: center"><img src="https://github.com/wtyhub/Photo/blob/a713229943f0628ffb82556bc9e396bbfabe8567/1%20inch.jpg?raw=true" width="160" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><a href="https://zdzheng.xyz">Zhedong Zheng</a>, National University of Singapore, Singapore</td>
      <td style="text-align: center"><a href="https://yujiaoshi.github.io/">Yujiao Shi</a>, ShanghaiTech University, China</td>
      <td style="text-align: center"><a href="https://scholar.google.com/citations?user=wv3H-F4AAAAJ">Tingyu Wang</a>, Hangzhou Dianzi University, China</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT__SOfspAhkPnw92bKFTX-g1FZcUMDYSuqPNTKtYulSw&amp;s" width="160" /></td>
      <td style="text-align: center"><img src="https://media.licdn.com/dms/image/D5603AQHq1FnQjYQH3g/profile-displayphoto-shrink_800_800/0/1703335806714?e=2147483647&amp;v=beta&amp;t=MMXjtt4dizU3fDxDsKGGLsAtuyJoUib3xn28NE1SulI" width="160" /></td>
      <td style="text-align: center"><img src="https://github.com/layumi/ACMMM2024Workshop-UAV/assets/8390471/812a43a1-b4fe-48f7-9a83-9e31ca861a75" width="160" /></td>
    </tr>
    <tr>
      <td style="text-align: center"><a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, University of Central Florida, USA</td>
      <td style="text-align: center"><a href="https://cic.tju.edu.cn/faculty/zhupengfei/index.html">Pengfei Zhu</a>, Tianjing University, China</td>
      <td style="text-align: center"><a href="https://users.cecs.anu.edu.au/~hartley/">Richard Hartley</a>, Australian National University, Australia</td>
    </tr>
  </tbody>
</table>

<h2 id="conference-and-journal-papers">Conference and Journal Papers</h2>

<p>All papers presented at ACMMM 2024 will be included in ACM proceeding. All papers submitted to this workshop will go through the same review process as the regular papers submitted to the main conference to ensure that the contributions are of high quality.</p>

<h2 id="student-traval-funding">Student Traval Funding</h2>

<p>Please check https://2024.acmmm.org/</p>

<h2 id="workshop-citation">Workshop Citation</h2>
<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zheng2024UVA</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{The 2nd Workshop on UAVs in Multimedia: Capturing the World from a New Perspective}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zhedong and Shi, Yujiao and Wang, Tingyu and Liu, Jun and Fang, Jianwu and Wei, Yunchao and Chua, Tat-seng}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the 32nd ACM International Conference on Multimedia Workshop}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>  
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="https://github.com/layumi"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://zdzheng.xyz/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Zhedong Zheng. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <!--
<script src="https://zdzheng.xyz/assets/js/main.min.js" async></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>




-->


  </body>
</html>

