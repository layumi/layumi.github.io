

<!doctype html>
<html lang="en" class="no-js">
  <head>
	<meta name="viewport" content="width=device-width, initial-scale=1">
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Pytorch ReID - Zhedong Zheng</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Zhedong Zheng">
<meta property="og:title" content="Pytorch ReID">








  <link rel="canonical" href="https://zdzheng.xyz/Pytorch-ReID">
  <meta property="og:url" content="https://zdzheng.xyz/Pytorch-ReID">























<!-- end SEO -->


<link href="https://zdzheng.xyz/feed.xml" type="application/atom+xml" rel="alternate" title="Zhedong Zheng Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<!--
<meta name="viewport" content="width=device-width, initial-scale=1.0">
-->
<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/main.css">

<!-- favicon -->
<link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">

<meta http-equiv="cleartype" content="on">

    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://zdzheng.xyz/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://zdzheng.xyz/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://zdzheng.xyz/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://zdzheng.xyz/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://zdzheng.xyz/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://zdzheng.xyz/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://zdzheng.xyz/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://zdzheng.xyz/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://zdzheng.xyz/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://zdzheng.xyz/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="mask-icon" href="https://zdzheng.xyz/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://zdzheng.xyz/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://zdzheng.xyz/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://zdzheng.xyz/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://zdzheng.xyz/">Zhedong Zheng</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/">Home</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/research">Research</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/files/zhedong-resume.pdf">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/resources/">Resources</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/seminar/">Seminar</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://zdzheng.xyz/recruitment/">Join Us</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





 
	<style>
		.author {
			text-decoration: none !important;
			color: #333333;
		}

		.author:hover {
			text-decoration: underline;
			color: #0066cc;
		}
	    pre {
	      background-color: gray;
	      color: white;
	      padding: 10px;
	      border-radius: 5px;
	    }
	</style>


<div id="main" role="main">
  <meta name="og:description" content="Pytorch ReID"> 
  



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Pytorch ReID">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Pytorch ReID
</h1>
          
        
        
        
        

        
		
		
		
		
			
		
		  <meta name="description" content="Pytorch ReID">
		
		
		
		
		
		
		
		
		
    
        </header>
      

      <section class="page__content" itemprop="text">
        <style>
table, th, td {
  border: 1px solid black;
}
</style>

<h1 align="center"> Pytorch ReID </h1>
<h2 align="center"> Strong, Small, Friendly </h2>

<p><img src="https://img.shields.io/badge/python-3.6+-green.svg" alt="Python3.6+">
<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License: MIT"></a></p>

<p>A tiny, friendly, strong baseline code for Object-reID (based on <a href="https://pytorch.org">pytorch</a>) since 2017.</p>

<p>Code is at https://github.com/layumi/Person_reID_baseline_pytorch .</p>

<ul>
  <li>
    <p><strong>Strong.</strong> It is consistent with the new baseline result in several top-conference works, e.g., <a href="https://arxiv.org/abs/1904.07223">Joint Discriminative and Generative Learning for Person Re-identification(CVPR19)</a>, <a href="https://arxiv.org/abs/1711.09349">Beyond Part Models: Person Retrieval with Refined Part Pooling(ECCV18)</a>, <a href="https://arxiv.org/abs/1711.10295">Camera Style Adaptation for Person Re-identification(CVPR18)</a>. We arrived Rank@1=88.24%, mAP=70.68% only with softmax loss.</p>
  </li>
  <li>
    <p><strong>Small.</strong> With fp16 (supported by Nvidia apex), our baseline could be trained with only 2GB GPU memory.</p>
  </li>
  <li>
    <p><strong>Friendly.</strong> You may use the off-the-shelf options to apply many state-of-the-art tricks in one line.
Besides, if you are new to object re-ID, you may check out our <strong><a href="https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/tutorial">Tutorial</a></strong> first (8 min read) <img class="emoji" title=":+1:" alt=":+1:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png" height="20" width="20"> .
<img src="https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/show.png?raw=true" alt="Person ReID Example"></p>
  </li>
</ul>

<p>Share to
    <!-- Facebook -->
    <a href="http://www.facebook.com/sharer.php?u=https://github.com/layumi/Person_reID_baseline_pytorch/" target="_blank">
        <img src="https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/1_Facebook_colored_svg_copy-64.webp" alt="Facebook" width="60">
    </a>
    <!-- Twitter -->
    <a href="https://twitter.com/share?url=https://github.com/layumi/Person_reID_baseline_pytorch/&amp;text=Strong,%20Small,%20Friendly%20Pytorch%20re-ID&amp;hashtags=pytorch%20re-ID" target="_blank">
        <img src="https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/2018_social_media_popular_app_logo_twitter-64.webp" alt="Twitter" width="60">
    </a>
    <!-- Weibo -->
    <a href="https://service.weibo.com/share/share.php?url=https://github.com/layumi/Person_reID_baseline_pytorch/&amp;text=Strong,%20Small,%20Friendly%20Pytorch%20re-ID&amp;hashtags=pytorch%20re-ID" target="_blank">
        <img src="https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/Weibo-64.webp" alt="Weibo" width="60">
    </a>
    <!-- LinkedIn -->
    <a href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/layumi/Person_reID_baseline_pytorch/" target="_blank">
        <img src="https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/circle-linkedin-64.webp" alt="LinkedIn" width="60">
    </a>
    <!-- Email -->
    <a href="mailto:?Subject=Strong,%20Small,%20Friendly%20Pytorch%20Re-identification&amp;Body=I%20saw%20this%20and%20thought%20of%20you!%20%20https://github.com/layumi/Person_reID_baseline_pytorch/">
        <img src="https://github.com/layumi/Person_reID_baseline_pytorch/raw/master/docs/Email-64.webp" alt="Email" width="60">
    </a></p>

<h2 id="tutorial">Tutorial</h2>
<ul>
  <li>
<a href="https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/tutorial/README.md">8 min Tutorial</a>，<a href="https://zhuanlan.zhihu.com/p/50387521">8分钟教程</a>
</li>
  <li><a href="https://www.bilibili.com/video/BV11K4y1f7eQ">中文视频简介</a></li>
  <li><a href="https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/tutorial/Answers_to_Quick_Questions.md">Answer to Tutorial</a></li>
</ul>

<h2 id="table-of-contents">Table of contents</h2>
<ul>
  <li><a href="#features">Features</a></li>
  <li><a href="#some-news">Some News</a></li>
  <li><a href="#trained-model">Trained Model</a></li>
  <li><a href="#prerequisites">Prerequisites</a></li>
  <li>
<a href="#getting-started">Getting Started</a>
    <ul>
      <li><a href="#installation">Installation</a></li>
      <li><a href="#dataset--preparation">Dataset Preparation</a></li>
      <li><a href="#train">Train</a></li>
      <li><a href="#test">Test</a></li>
      <li><a href="#evaluation">Evaluation</a></li>
    </ul>
  </li>
  <li><a href="#tips">Tips for training with other datasets</a></li>
  <li><a href="#citation">How to Cite?</a></li>
  <li><a href="#related-repos">Related Repos</a></li>
</ul>

<h2 id="features">Features</h2>
<p>Now we have supported:</p>

<h3 id="training">Training</h3>
<ul>
  <li>Running the code on Google Colab with Free GPU. Check <a href="https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/colab">Here</a> (Thanks to @ronghao233)</li>
  <li>
<a href="https://github.com/NVlabs/DG-Net#dg-market">DG-Market</a> (10x Large Synethic Dataset from Market <strong>CVPR 2019 Oral</strong>)</li>
  <li>
<a href="https://github.com/microsoft/Swin-Transformer">Swin Transformer</a> / <a href="https://github.com/lukemelas/EfficientNet-PyTorch">EfficientNet</a> / <a href="https://github.com/HRNet">HRNet</a>
</li>
  <li>ResNet/ResNet-ibn/DenseNet</li>
  <li>Circle Loss, Triplet Loss, Contrastive Loss, Sphere Loss, Lifted Loss, Arcface, Cosface  and Instance Loss</li>
  <li>Float16 to save GPU memory based on <a href="https://github.com/NVIDIA/apex">apex</a>
</li>
  <li>Part-based Convolutional Baseline(PCB)</li>
  <li>Random Erasing</li>
  <li>Linear Warm-up</li>
</ul>

<h3 id="testing">Testing</h3>
<ul>
  <li>TensorRT</li>
  <li>Pytorch JIT</li>
  <li>Fuse Conv and BN layer into one Conv layer</li>
  <li>Multiple Query Evaluation</li>
  <li>Re-Ranking (CPU &amp; <a href="https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/GPU-Re-Ranking">GPU Version</a>)</li>
  <li>Visualize Training Curves</li>
  <li>Visualize Ranking Result</li>
  <li><a href="https://github.com/layumi/Person_reID_baseline_pytorch/blob/dev/visual_heatmap.py">Visualize Heatmap</a></li>
</ul>

<p>Here we provide hyperparameters and architectures, that were used to generate the result. 
Some of them (i.e. learning rate) are far from optimal. Do not hesitate to change them and see the effect.</p>

<p>P.S. With similar structure, we arrived <strong>Rank@1=87.74% mAP=69.46%</strong> with <a href="http://www.vlfeat.org/matconvnet/">Matconvnet</a>. (batchsize=8, dropout=0.75) 
You may refer to <a href="https://github.com/layumi/Person_reID_baseline_matconvnet">Here</a>.
Different framework need to be tuned in a different way.</p>

<h2 id="some-news">Some News</h2>

<p><strong>12 Aug 2023</strong> Large Person Langauge Model is currently available at <a href="https://github.com/Shuyu-XJTU/APTM">Here</a><img src="https://img.shields.io/github/stars/Shuyu-XJTU/APTM.svg?style=flat&amp;label=Star" alt="GitHub stars"> accepted by ACM MM’23. You are welcomed to check it.</p>

<p><strong>19 Mar 2023</strong> We host a special session on IEEE Intelligent Transportation Systems Conference (ITSC), covering the object re-identification &amp; point cloud topic. The paper ddl is by May 15, 2023 and the paper notification is at June 30, 2023. Please select the session code ``w7r4a’’ during submission. More details can be found at <a href="https://2023.ieee-itsc.org/wp-content/uploads/2023/03/IEEE-ITSC-2023-Special-Session-Proposal-Safe-Critical-Scenario-Understanding-in-Intelligent-Transportation-Systems-SCSU-ITS.pdf">Special Session Website</a>.</p>

<p><strong>9 Mar 2023</strong> Market-1501 is in 3D. Please check our single 2D to 3D reconstruction work  https://github.com/layumi/3D-Magic-Mirror <img src="https://img.shields.io/github/stars/layumi/3D-Magic-Mirror.svg?style=flat&amp;label=Star" alt="GitHub stars">.
<img src="https://github.com/layumi/3D-Magic-Mirror/raw/master/doc/rainbow_github.gif" alt="3D Human Animation"></p>

<details>
 <summary><b>
  2022 News
</b></summary>

**7 Sep 2022** We support SwinV2. 

**24 Jul 2022** Market-HQ is released with super-resolution quality from 128\*64 to 512\*256. Please check at https://github.com/layumi/HQ-Market

**14 Jul 2022** Add adversarial training by ``python train.py --name ftnet_adv --adv 0.1 --aiter 40``. 

**1 Feb 2022** Speed up the inference process about 10 seconds by removing the ``cat`` function in ``test.py``. 
   
**1 Feb 2022** Add the demo with ``TensorRT`` (The fast inference speed may depend on the GPU with the latest RT Core).
   
</details>

<details>
 <summary><b>
  2021 News
 </b></summary>

**30 Dec 2021** We add supports for new losses, including arcface loss, cosface loss and instance loss. The hyper-parameters are still tunning.
   
**3 Dec 2021** We add supports for four losses, including triplet loss, contrastive loss, sphere loss and lifted loss. The hyper-parameters are still tunning.
   
**1 Dec 2021** We support EfficientNet/HRNet.
   
**15 Sep 2021** We support ResNet-ibn from ECCV2018 (https://github.com/XingangPan/IBN-Net). 

**17 Aug 2021** We support running code on Google Colab with free GPU. Please check it out at https://github.com/layumi/Person_reID_baseline_pytorch/tree/master/colab .

**14 Aug 2021** We have supported the training with [DG-Market](https://github.com/NVlabs/DG-Net#dg-market) for regularization via [Self-supervised Memory Learning](https://www.ijcai.org/proceedings/2020/150). You only neeed to download/unzip the dataset and add `--DG` to train model. 

**12 Aug 2021** We have supported the transformer-based model `Swin` by `--use_swin`. The basic performance is 92.73% Rank@1 and 79.71%mAP.

**23 Jun 2021** Attack your re-ID model via Query! They are not robust as you expected! Check the code at [Here](https://github.com/layumi/A_reID).

**5 Feb 2021** We have supported [Circle loss](https://arxiv.org/abs/2002.10857)(CVPR20 Oral). You can try it by simply adding `--circle`.  

**11 January 2021** On the Market-1501 dataset, we accelerate the re-ranking processing from **89.2s** to **9.4ms** with one K40m GPU, facilitating the real-time post-processing. The pytorch implementation can be found in [GPU-Re-Ranking](GPU-Re-Ranking/).

</details>

<details>
 <summary><b>
  2020 News
 </b></summary>
   
**11 June 2020** People live in the 3D world. We release one new person re-id code [Person Re-identification in the 3D Space](https://github.com/layumi/person-reid-3d), which conduct representation learning in the 3D space. You are welcomed to check out it.

<img width="250" height="150" src="https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/pdf/3D-demo.png">


**30 April 2020** We have applied this code to the [AICity Challenge 2020](https://www.aicitychallenge.org/),  yielding the 1st Place Submission to the re-id track <img class="emoji" title=":red_car:" alt=":red_car:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f697.png" height="20" width="20">. Check out [here](https://github.com/layumi/AICIty-reID-2020).

**01 March 2020** We release one new image retrieval dataset, called [University-1652](https://github.com/layumi/University1652-Baseline), for drone-view target localization and drone navigation <img class="emoji" title=":helicopter:" alt=":helicopter:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f681.png" height="20" width="20">. It has a similar setting with the person re-ID. You are welcomed to check out it.
</details>

<details>
 <summary><b>
  2019 News
 </b></summary>
   
**07 July 2019:** I added some new functions, such as `--resume`, auto-augmentation policy, acos loss, into [developing thread](https://github.com/layumi/Person_reID_baseline_pytorch/tree/dev) and rewrite the `save` and `load` functions. I haven't tested the functions throughly. Some new functions are worthy of having a try. If you are first to this repo, I suggest you stay with the master thread.

**01 July 2019:** [My CVPR19 Paper](https://arxiv.org/abs/1904.07223) is online. It is based on this baseline repo as teacher model to provide pseudo label for the generated images to train a better student model. You are welcomed to check out the opensource code at [here](https://github.com/NVlabs/DG-Net).

**03 Jun 2019:** Testing with multiple-scale inputs is added. You can use `--ms 1,0.9` when extracting the feature. It could slightly improve the final result.

**20 May 2019:** Linear Warm Up is added. You also can set warm-up the first K epoch by `--warm_epoch K`. If K &lt;=0, there will be no warm-up.

</details>

<details>
 <summary><b>
  2018 &amp; 2017 News
 </b></summary> 
   
**What's new:** FP16 has been added. It can be used by simply added `--fp16`. You need to install [apex](https://github.com/NVIDIA/apex) and update your pytorch to 1.0. 

Float16 could save about 50% GPU memory usage without accuracy drop. **Our baseline could be trained with only 2GB GPU memory.** 
```bash
python train.py --fp16
```
**What's new:** Visualizing ranking result is added.
```bash
python prepare.py
python train.py
python test.py
python demo.py --query_index 777
```

**What's new:** Multiple-query Evaluation is added. The multiple-query result is about **Rank@1=91.95% mAP=78.06%**. 
```bash
python prepare.py
python train.py
python test.py --multi
python evaluate_gpu.py
```

**What's new:**  [PCB](https://arxiv.org/abs/1711.09349) is added. You may use '--PCB' to use this model. It can achieve around **Rank@1=92.73% mAP=78.16%**. I used a GPU (P40) with 24GB Memory. You may try apply smaller batchsize and choose the smaller learning rate (for stability) to run. (For example, `--batchsize 32 --lr 0.01 --PCB`)
```bash
python train.py --PCB --batchsize 64 --name PCB-64
python test.py --PCB --name PCB-64
```

**What's new:** You may try `evaluate_gpu.py` to conduct a faster evaluation with GPU.

**What's new:** You may apply '--use_dense' to use `DenseNet-121`. It can arrive around Rank@1=89.91% mAP=73.58%. 

**What's new:** Re-ranking is added to evaluation. The re-ranked result is about **Rank@1=90.20% mAP=84.76%**.

**What's new:** Random Erasing is added to train.

**What's new:** I add some code to generate training curves. The figure will be saved into the model folder when training.

![](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/train.jpg)
</details>

<h2 id="trained-model">Trained Model</h2>

<p>I re-trained several models, and the results may be different with the original one. Just for a quick reference, you may directly use these models. The download link is <a href="https://drive.google.com/open?id=1XVEYb0TN2SbBYOqf8SzazfYZlpH9CxyE">Here</a>.</p>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Rank@1</th>
      <th>mAP</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>[EfficientNet-b4]</td>
      <td>85.78%</td>
      <td>66.80%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_efficient --name eff; python test.py --name eff</code></td>
    </tr>
    <tr>
      <td>[ResNet-50 + adv defense]</td>
      <td>87.77%</td>
      <td>69.83%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py  --name adv0.1_40_w10_all --adv 0.1 --aiter 40 --warm 10 --train_all; python test.py  --name adv0.1_40_w10_all</code></td>
    </tr>
    <tr>
      <td>[ConvNeXt]</td>
      <td>88.98%</td>
      <td>71.35%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_convnext --name convnext; python test.py --name convnext</code></td>
    </tr>
    <tr>
      <td>[ResNet-50 (fp16)]</td>
      <td>88.03%</td>
      <td>71.40%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --name fp16 --fp16 --train_all</code></td>
    </tr>
    <tr>
      <td>[ResNet-50]</td>
      <td>88.84%</td>
      <td>71.59%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --train_all</code></td>
    </tr>
    <tr>
      <td>[ResNet-50-ibn]</td>
      <td>89.13%</td>
      <td>73.40%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --train_all --name res-ibn --ibn</code></td>
    </tr>
    <tr>
      <td>[DenseNet-121]</td>
      <td>90.17%</td>
      <td>74.02%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --name ft_net_dense --use_dense --train_all</code></td>
    </tr>
    <tr>
      <td>[DenseNet-121 (Circle)]</td>
      <td>91.00%</td>
      <td>76.54%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --name ft_net_dense_circle_w5 --circle --use_dense --train_all --warm_epoch 5</code></td>
    </tr>
    <tr>
      <td>[HRNet-18]</td>
      <td>90.83%</td>
      <td>76.65%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_hr --name hr18; python test.py --name hr18</code></td>
    </tr>
    <tr>
      <td>[PCB]</td>
      <td>92.64%</td>
      <td>77.47%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --name PCB --PCB --train_all --lr 0.02</code></td>
    </tr>
    <tr>
      <td>[PCB + DG]</td>
      <td>92.70%</td>
      <td>78.31%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --name PCB_DG --PCB --train_all --lr 0.02 --DG; python test.py --name PCB_DG</code></td>
    </tr>
    <tr>
      <td>[ResNet-50 (all tricks)]</td>
      <td>91.83%</td>
      <td>78.32%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5</code></td>
    </tr>
    <tr>
      <td>[ResNet-50 (all tricks+Circle)]</td>
      <td>92.13%</td>
      <td>79.84%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle  --circle</code></td>
    </tr>
    <tr>
      <td>[ResNet-50 (all tricks+Circle+DG)]</td>
      <td>92.13%</td>
      <td>80.13%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name warm5_s1_b8_lr2_p0.5_circle_DG --circle --DG; python test.py --name warm5_s1_b8_lr2_p0.5_circle_DG</code></td>
    </tr>
    <tr>
      <td>[DenseNet-121 (all tricks+Circle)]</td>
      <td>92.61%</td>
      <td>80.24%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 8 --lr 0.02 --name dense_warm5_s1_b8_lr2_p0.5_circle --circle --use_dense; python test.py --name  dense_warm5_s1_b8_lr2_p0.5_circle</code></td>
    </tr>
    <tr>
      <td>[HRNet-18 (all tricks+Circle+DG)]</td>
      <td>92.19%</td>
      <td>81.00%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_hr --name  hr18_p0.5_circle_w5_b16_lr0.01_DG --lr 0.01 --batch 16 --DG --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name  hr18_p0.5_circle_w5_b16_lr0.01_DG</code></td>
    </tr>
    <tr>
      <td>[Swin] (224x224)</td>
      <td>92.75%</td>
      <td>79.70%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_swin --name swin; python test.py --name swin</code></td>
    </tr>
    <tr>
      <td>[SwinV2 (all tricks+Circle 256x128)]</td>
      <td>92.93%</td>
      <td>82.99%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_swinv2 --name swinv2_p0.5_circle_w5_b16_lr0.03  --lr 0.03 --batch 16 --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name   swinv2_p0.5_circle_w5_b16_lr0.03 --batch 32</code></td>
    </tr>
    <tr>
      <td>[Swin (all tricks+Circle 224x224)]</td>
      <td>94.12%</td>
      <td>84.39%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_swin --name swin_p0.5_circle_w5  --erasing_p 0.5 --circle --warm_epoch 5;  python test.py --name swin_p0.5_circle_w5</code></td>
    </tr>
    <tr>
      <td>[Swin (all tricks+Circle+b16 224x224)]</td>
      <td>94.00%</td>
      <td>85.21%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_swin --name swin_p0.5_circle_w5_b16_lr0.01 --lr 0.01 --batch 16  --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name swin_p0.5_circle_w5_b16_lr0.01</code></td>
    </tr>
    <tr>
      <td>[Swin (all tricks+Circle+b16+DG 224x224)]</td>
      <td>94.00%</td>
      <td>85.36%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --use_swin --name swin_p0.5_circle_w5_b16_lr0.01_DG --lr 0.01 --batch 16 --DG --erasing_p 0.5 --circle --warm_epoch 5; python test.py --name swin_p0.5_circle_w5_b16_lr0.01_DG</code></td>
    </tr>
  </tbody>
</table>

<ul>
  <li>More training iterations may lead to better results.</li>
  <li>Swin costs more GPU memory (11G GPU is needed) to run.</li>
  <li>The hyper-parameter of <a href="https://github.com/NVlabs/DG-Net#dg-market">DG-Market</a> <code class="language-plaintext highlighter-rouge">--DG</code> is not tuned. Better hyper-parameter may lead to better results.</li>
</ul>

<h3 id="different-losses">Different Losses</h3>

<p>I do not optimize the hyper-parameters. You are free to tune them for better performance.</p>

<table>
  <thead>
    <tr>
      <th>Methods</th>
      <th>Rank@1</th>
      <th>mAP</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CE</td>
      <td>92.01%</td>
      <td>79.31%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_100 --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_100</code></td>
    </tr>
    <tr>
      <td>CE + Sphere <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf">[Paper]</a>
</td>
      <td>92.01%</td>
      <td>79.39%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_sphere100 --sphere --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_sphere100</code></td>
    </tr>
    <tr>
      <td>CE + Triplet <a href="https://arxiv.org/pdf/1703.07737">[Paper]</a>
</td>
      <td>92.40%</td>
      <td>79.71%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_triplet100 --triplet --total 100; python test.py  --name warm5_s1_b32_lr8_p0.5_triplet100</code></td>
    </tr>
    <tr>
      <td>CE + Lifted <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf">[Paper]</a>
</td>
      <td>91.78%</td>
      <td>79.77%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_lifted100 --lifted --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_lifted100</code></td>
    </tr>
    <tr>
      <td>CE + Instance <a href="https://zdzheng.xyz/files/TOMM20.pdf">[Paper]</a>
</td>
      <td>92.73%</td>
      <td>81.11%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_instance100_gamma64 --instance --ins_gamma 64 --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_instance100_gamma64</code></td>
    </tr>
    <tr>
      <td>CE + Contrast <a href="https://zdzheng.xyz/files/TOMM18.pdf">[Paper]</a>
</td>
      <td>92.28%</td>
      <td>81.42%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_contrast100 --contrast  --total 100; python test.py  --name warm5_s1_b32_lr8_p0.5_contrast100</code></td>
    </tr>
    <tr>
      <td>CE + Circle <a href="https://arxiv.org/abs/2002.10857">[Paper]</a>
</td>
      <td>92.46%</td>
      <td>81.70%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_circle100 --circle --total 100 ; python test.py  --name  warm5_s1_b32_lr8_p0.5_circle100</code></td>
    </tr>
    <tr>
      <td>CE + Contrast + Sphere</td>
      <td>92.79%</td>
      <td>82.02%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 32 --lr 0.08 --name warm5_s1_b32_lr8_p0.5_cs100 --contrast --sphere --total 100; python test.py --name warm5_s1_b32_lr8_p0.5_cs100</code></td>
    </tr>
    <tr>
      <td>CE + Contrast + Triplet (Long)</td>
      <td>92.61%</td>
      <td>82.01%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.062 --name warm5_s1_b24_lr6.2_p0.5_contrast_triplet_133 --contrast --triplet --total 133 ; python test.py  --name  warm5_s1_b24_lr6.2_p0.5_contrast_triplet_133</code></td>
    </tr>
    <tr>
      <td>CE + Contrast + Circle (Long)</td>
      <td>92.19%</td>
      <td>82.07%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.08 --name warm5_s1_b24_lr8_p0.5_contrast_circle133 --contrast --circle --total 133 ; python test.py  --name  warm5_s1_b24_lr8_p0.5_contrast_circle133</code></td>
    </tr>
    <tr>
      <td>CE + Contrast + Sphere (Long)</td>
      <td>92.84%</td>
      <td>82.37%</td>
      <td><code class="language-plaintext highlighter-rouge">python train.py --warm_epoch 5 --stride 1 --erasing_p 0.5 --batchsize 24 --lr 0.06 --name warm5_s1_b24_lr6_p0.5_contrast_sphere133 --contrast --sphere --total 133 ; python test.py  --name  warm5_s1_b24_lr6_p0.5_contrast_sphere133</code></td>
    </tr>
  </tbody>
</table>

<h3 id="model-structure">Model Structure</h3>

<p>You may learn more from <code class="language-plaintext highlighter-rouge">model.py</code>. 
We add one linear layer(bottleneck), one batchnorm layer and relu.</p>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>Python 3.6+</li>
  <li>GPU Memory &gt;= 6G</li>
  <li>Numpy</li>
  <li>Pytorch 0.3+</li>
  <li>timm <code class="language-plaintext highlighter-rouge">pip install timm</code> for Swin-Transformer with Pytorch &gt;1.7.0</li>
  <li>pretrainedmodels via <code class="language-plaintext highlighter-rouge">pip install pretrainedmodels</code>
</li>
  <li>[Optional] apex (for float16)</li>
  <li>[Optional] <a href="https://github.com/Cadene/pretrained-models.pytorch">pretrainedmodels</a>
</li>
</ul>

<p><strong>(Some reports found that updating numpy can arrive the right accuracy. If you only get 50~80 Top1 Accuracy, just try it.)</strong>
We have successfully run the code based on numpy 1.12.1 and 1.13.1 .</p>

<h2 id="getting-started">Getting started</h2>
<h3 id="installation">Installation</h3>
<ul>
  <li>Install Pytorch from http://pytorch.org/</li>
  <li>Install Torchvision from the source
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>git clone https://github.com/pytorch/vision
cd vision
python setup.py install
</code></pre></div>    </div>
  </li>
  <li>[Optional] You may skip it. Install apex from the source
    <div class="language-plaintext highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>git clone https://github.com/NVIDIA/apex.git
cd apex
python setup.py install --cuda_ext --cpp_ext
</code></pre></div>    </div>
    <p>Because pytorch and torchvision are ongoing projects.</p>
  </li>
</ul>

<p>Here we noted that our code is tested based on Pytorch 0.3.0/0.4.0/0.5.0/1.0.0 and Torchvision 0.2.0/0.2.1 .</p>

<h3 id="dataset--preparation">Dataset &amp; Preparation</h3>

<p>Download <a href="https://zheng-lab.cecs.anu.edu.au/Project/project_reid.html">Market1501 Dataset</a> <a href="https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view">[Google]</a> <a href="https://pan.baidu.com/s/1ntIi2Op">[Baidu]</a> Or use command line:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gdown 
pip <span class="nb">install</span> <span class="nt">--upgrade</span> gdown <span class="c">#!!important!!</span>
gdown 0B8-rUzbwVRk0c054eEozWG9COHM
</code></pre></div></div>

<p>Preparation: Put the images with the same id in one folder. You may use</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python prepare.py
</code></pre></div></div>
<p>Remember to change the dataset path to your own path.</p>

<p>Futhermore, you also can test our code on DukeMTMC-reID Dataset( <a href="https://drive.google.com/open?id=1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O">GoogleDriver</a> or (<a href="https://pan.baidu.com/s/1jS0XM7Var5nQGcbf9xUztw">BaiduYun</a> password: bhbh)) Or use command line:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gdown 1jjE85dRCMOgRtvJ5RQV9-Afs-2_5dY3O
</code></pre></div></div>
<p>Our baseline code is not such high on DukeMTMC-reID <strong>Rank@1=64.23%, mAP=43.92%</strong>. Hyperparameters are need to be tuned.</p>

<ul>
  <li>[Optional] <a href="https://github.com/NVlabs/DG-Net#dg-market">DG-Market</a> is a generated pedestrian dataset of 128,307 images for training a robust model.</li>
</ul>

<h3 id="train">Train</h3>
<p>Train a model by</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py <span class="nt">--gpu_ids</span> 0 <span class="nt">--name</span> ft_ResNet50 <span class="nt">--train_all</span> <span class="nt">--batchsize</span> 32  <span class="nt">--data_dir</span> your_data_path
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">--gpu_ids</code> which gpu to run.</p>

<p><code class="language-plaintext highlighter-rouge">--name</code> the name of model.</p>

<p><code class="language-plaintext highlighter-rouge">--data_dir</code> the path of the training data.</p>

<p><code class="language-plaintext highlighter-rouge">--train_all</code> using all images to train.</p>

<p><code class="language-plaintext highlighter-rouge">--batchsize</code> batch size.</p>

<p><code class="language-plaintext highlighter-rouge">--erasing_p</code> random erasing probability.</p>

<p>Train a model with random erasing by</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py <span class="nt">--gpu_ids</span> 0 <span class="nt">--name</span> ft_ResNet50 <span class="nt">--train_all</span> <span class="nt">--batchsize</span> 32  <span class="nt">--data_dir</span> your_data_path <span class="nt">--erasing_p</span> 0.5
</code></pre></div></div>

<h3 id="test">Test</h3>
<p>Use trained model to extract feature by</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python test.py <span class="nt">--gpu_ids</span> 0 <span class="nt">--name</span> ft_ResNet50 <span class="nt">--test_dir</span> your_data_path  <span class="nt">--batchsize</span> 32 <span class="nt">--which_epoch</span> 59
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">--gpu_ids</code> which gpu to run.</p>

<p><code class="language-plaintext highlighter-rouge">--batchsize</code> batch size.</p>

<p><code class="language-plaintext highlighter-rouge">--name</code> the dir name of trained model.</p>

<p><code class="language-plaintext highlighter-rouge">--which_epoch</code> select the i-th model.</p>

<p><code class="language-plaintext highlighter-rouge">--data_dir</code> the path of the testing data.</p>

<h3 id="evaluation">Evaluation</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python evaluate.py
</code></pre></div></div>
<p>It will output Rank@1, Rank@5, Rank@10 and mAP results.
You may also try <code class="language-plaintext highlighter-rouge">evaluate_gpu.py</code> to conduct a faster evaluation with GPU.</p>

<p>For mAP calculation, you also can refer to the <a href="http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp">C++ code for Oxford Building</a>. We use the triangle mAP calculation (consistent with the Market1501 original code).</p>

<h3 id="re-ranking">re-ranking</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python evaluate_rerank.py
</code></pre></div></div>
<p><strong>It may take more than 10G Memory to run.</strong> So run it on a powerful machine if possible.</p>

<p>It will output Rank@1, Rank@5, Rank@10 and mAP results.</p>

<h3 id="tips">Tips</h3>
<p>Notes the format of the camera id and the number of cameras.</p>

<p>For some dataset, e.g., MSMT17, there are more than 10 cameras. You need to modify the <code class="language-plaintext highlighter-rouge">prepare.py</code> and <code class="language-plaintext highlighter-rouge">test.py</code> to read the double-digit camera ID.</p>

<p>For some vehicle re-ID datasets. e.g. VeRi, you also need to modify the <code class="language-plaintext highlighter-rouge">prepare.py</code> and <code class="language-plaintext highlighter-rouge">test.py</code>.  It has different naming rules.
https://github.com/layumi/Person_reID_baseline_pytorch/issues/107 (Sorry. It is in Chinese)</p>

<h2 id="citation">Citation</h2>
<p>The following paper uses and reports the result of the baseline model. You may cite it in your paper.</p>
<div class="language-bib highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zheng2019joint</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Joint discriminative and generative learning for person re-identification}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zhedong and Yang, Xiaodong and Yu, Zhiding and Zheng, Liang and Yang, Yi and Kautz, Jan}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The following papers may be the first two to use the bottleneck baseline. You may cite them in your paper.</p>
<div class="language-bib highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/SunZDW17</span><span class="p">,</span>
  <span class="na">author</span>    <span class="p">=</span> <span class="s">{Yifan Sun and
               Liang Zheng and
               Weijian Deng and
               Shengjin Wang}</span><span class="p">,</span>
  <span class="na">title</span>     <span class="p">=</span> <span class="s">{SVDNet for Pedestrian Retrieval}</span><span class="p">,</span>
  <span class="na">booktitle</span>   <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">year</span>      <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">hermans2017defense</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{In Defense of the Triplet Loss for Person Re-Identification}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Hermans, Alexander and Beyer, Lucas and Leibe, Bastian}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:1703.07737}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2017}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Basic Model</p>
<div class="language-bib highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zheng2018discriminatively</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{A discriminatively learned CNN embedding for person reidentification}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zhedong and Zheng, Liang and Yang, Yi}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{13}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2018}</span><span class="p">,</span>
  <span class="na">publisher</span><span class="p">=</span><span class="s">{ACM}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">zheng2020vehiclenet</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{VehicleNet: Learning Robust Visual Representation for Vehicle Re-identification}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Zheng, Zhedong and Ruan, Tao and Wei, Yunchao and Yang, Yi and Mei, Tao}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{IEEE Transaction on Multimedia (TMM)}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2020}</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="related-repos">Related Repos</h2>
<ol>
  <li>
<a href="https://github.com/layumi/Pedestrian_Alignment">Pedestrian Alignment Network</a> <img src="https://img.shields.io/github/stars/layumi/Pedestrian_Alignment.svg?style=flat&amp;label=Star" alt="GitHub stars">
</li>
  <li>
<a href="https://github.com/layumi/2016_person_re-ID">2stream Person re-ID</a> <img src="https://img.shields.io/github/stars/layumi/2016_person_re-ID.svg?style=flat&amp;label=Star" alt="GitHub stars">
</li>
  <li>
<a href="https://github.com/layumi/Person-reID_GAN">Pedestrian GAN</a> <img src="https://img.shields.io/github/stars/layumi/Person-reID_GAN.svg?style=flat&amp;label=Star" alt="GitHub stars">
</li>
  <li>
<a href="https://github.com/layumi/Image-Text-Embedding">Language Person Search</a> <img src="https://img.shields.io/github/stars/layumi/Image-Text-Embedding.svg?style=flat&amp;label=Star" alt="GitHub stars">
</li>
  <li>
<a href="https://github.com/NVlabs/DG-Net">DG-Net</a> <img src="https://img.shields.io/github/stars/NVlabs/DG-Net.svg?style=flat&amp;label=Star" alt="GitHub stars">
</li>
  <li>
<a href="https://github.com/layumi/person-reid-3d">3D Person re-ID</a> <img src="https://img.shields.io/github/stars/layumi/person-reid-3d.svg?style=flat&amp;label=Star" alt="GitHub stars">
</li>
</ol>

        
      </section>

      <footer class="page__meta">
        
        




      </footer>

      

      


    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="https://github.com/layumi"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://zdzheng.xyz/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">© 2024 Zhedong Zheng. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <!--
<script src="https://zdzheng.xyz/assets/js/main.min.js" async></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>




-->


  </body>
</html>

