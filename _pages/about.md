---
permalink: /
title: "Zhedong Zheng (郑哲东)"
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
---

<meta name="description"
  content="Zhedong Zheng is a distinguished postdoctoral research fellow at the National University of Singapore, specializing in person re-identification (reID). He obtained his Ph.D. from UTS's ReLER Lab, mentored by Prof. Yi Yang and Dr. Liang Zheng. With a strong foundation from Fudan University, he has collaborated with leading experts at Nvidia and Baidu. His focus on reID underscores his dedication to advancing this vital field.">
  
<meta name="keywords" content="Zhedong Zheng, Person Re-ID, Object Re-ID, Person Retrieval, Person Search" />
		   
Hi! I am currently a tenure track assistant professor with the University of Macau. Prior to this, I was postdoctoral research fellow at School of Computing, National University of Singapore with <a href="https://www.comp.nus.edu.sg/cs/bio/chuats/">Prof. Tat-Seng Chua</a> and <a href="https://www.comp.nus.edu.sg/~ayao/">Prof. Angela Yao</a>. I received Ph.D. from the <a href="https://reler.net/">ReLER Lab</a>, <a href="https://www.uts.edu.au/">University of Technology Sydney (UTS) </a>, under the supervision of <a href="https://scholar.google.com/citations?user=RMSuNFwAAAAJ">Prof. Yi Yang</a> and <a href="https://zheng-lab.cecs.anu.edu.au/">Dr. Liang Zheng</a> (co-supervisor). 
Before that, I completed my Bachelor's degree from <a href="https://www.fudan.edu.cn">Fudan University</a> in 2016, under the supervision of <a href="https://scholar.google.com.au/citations?user=DTbhX6oAAAAJ&hl=en">Prof. Xiangyang Xue</a>. 
Throughout my academic journey, I have been fortunate to collaborate with several talented researchers, <a href='https://xiaodongyang.org/'>Xiaodong Yang</a> (Nvida), <a href='https://chrisding.github.io/'>Zhiding Yu</a> (Nvidia), <a href='https://jankautz.com/'>Jan Kautz</a> (Nvidia), <a href='https://github.com/miraclebiu'>Minyue Jiang</a> (Baidu) and <a href='https://scholar.google.com/citations?user=R1rVRUkAAAAJ'>Xiao Tan</a> (Baidu). 

Open-source projects can be found at my <a href='https://github.com/layumi'>[Github]</a>, and publications can be found at  [SCI](https://www.webofscience.com/wos/author/record/434956), [SCOPUS](https://www.scopus.com/authid/detail.uri?authorId=57200174037), or [Google Scholar](https://scholar.google.com/citations?hl=en&user=XT17oUEAAAAJ).

<strong>More details can be found in my [[CV]]({{ site.url }}{{ site.baseurl }}/files/zhedong-resume.pdf). </strong>

<!---
<ul>
<li> <mark>If you are a NUS Undergraduate / Master student who is interested in doing research/project with me, please contact me via email with your CV. My current email address is zdzheng AT nus.edu.sg . We will have at least two mentors to guide you and provide gpu resource supports.</mark></li> 
</ul>
-->	
<hr>

<h2>Research Statement</h2>

My work focuses on multi-view object matching and novel view synthesis and analysis, under a multi-camera scenario, e.g., swarm robotics, self-driving car, and smart city.

The child understands a 3D object, not from a still image, but from multiple images / videos capturing the target of interest from different viewpoints.

<!---
Big data is the primary part of training data-driven models. There remain three scientific questions. 

- Data Generation:  How to obtain more data? Due to the annotation costs and privacy concerns,  we usually could not access the large-scale data easily.  

- Prior Knowledge:  Does more data mean a better model? Deep learning also demands a deep understanding of data (robustness and explainability).

- Efficiency:  How to train on million-scale data? What data matters most? Efficient training and inference is needed.  

AI is not when a computer can write poetry. AI is when a computer want to write poetry. 
-->
<hr>

<h2>News</h2>
<ul>
<li> Two papers to appear at ACM Multimedia 2023 on Text-image re-ID <a href="https://www.zdzheng.xyz/publication/Towards-2023">[link]</a> and Domain Adaptation <a href="https://www.zdzheng.xyz/publication/PiPa-Pix2023">[link]</a></li>
<li> We are holding two workshops at ACM Multimedia 2023 on Aerial-view Imaging <a href="https://zdzheng.xyz/ACMMM2023Workshop/">[Call for papers]</a> and Deep Multimodal Learning <a href="https://videorelation.nextcenter.org/MMIR23/">[Call for papers]</a> (EI-indexed).   </li>
<li> Two papers to appear at CVPR 2023. </li>
<li> One paper on Adversarial Retrieval Attacking is accepted by IJCV 2022. <a href="https://zdzheng.xyz/publication/U-turn-C2022">[link]</a></li> 
<li> One paper on 3D Human Re-id is accepted by TNNLS 2022. <a href="https://zdzheng.xyz/publication/Paramete2022">[link]</a></li> 
<li> One AdaBoost Domain Adaptation paper is accepted by TIP 2022. <a href="https://zdzheng.xyz/publication/Adaptive2022">[link]</a></li> 
<li> One Drone-based Geolocalization paper is accepted by TIP 2022. <a href="https://zdzheng.xyz/publication/Joint-Re2022">[link]</a></li> 
<li> One Nerf paper to appear at CVPR 2022. <a href="https://zdzheng.xyz/publication/Multi-Vi2022">[link]</a></li> 
<li> We are holding the special session at ICME 2022 on Beyond Accuracy: Responsible, Responsive, and Robust Multimedia Retrieval. <a href="https://zdzheng.xyz/ICME2022SS/">[Call for papers]</a>  </li>
<li> My Ph.D. thesis was on the Chancellor's List. </li>
<li> I was awarded 2021 IEEE Circuits and Systems Society Outstanding Young Author Award. Thanks a lot for  supports and understanding from my supervisors and friends. <a href="https://ieee-cas.org/award/outstanding-paper-awards/outstanding-young-author-award#recipients">[link]</a> </li>
<li> One paper on Uncertainty is accepted by IJCV 2021. 
	<a href="https://zdzheng.xyz/publication/Recti2021">[PDF]</a> <a href="https://github.com/layumi/Seg_Uncertainty">[code]</a> </li>
<li> We have released a new Drone-view Geo-localization Dataset, ACM Multimedia 2020.<strong> 
	<a href="https://zdzheng.xyz/publication/Unive2020">[PDF]</a>
	<a href="https://github.com/layumi/University1652-Baseline">[Dataset]</a> <a href="https://www.youtube.com/embed/dzxXPp8tVn4?vq=hd1080">[Video]</a> <a href="https://zdzheng.xyz/files/ACM-MM-Talk.pdf">[Slide]</a></strong> </li>
<li> People live in a 3D world. Why not conduct representation learning in the 3D space? <a href="https://arxiv.org/abs/2006.04569">[arXiv]</a> <a href="https://github.com/layumi/person-reid-3d">[code]</a></li>
<li> We have achieved the <strong>1st</strong> place in AICity Challenge Vehicle Re-id Track, CVPR 2020. <a href="https://github.com/layumi/AICIty-reID-2020">[code] </a></li>
<li> Two papers to appear at IJCAI 2020. <a href="https://zdzheng.xyz/publication/Unsup2020">[PDF1]</a><a href="https://github.com/layumi/Seg_Uncertainty">[code1]</a><a href="https://zdzheng.xyz/publication/Real-2020">[PDF2]</a>
	<a href="https://github.com/huangzhikun1995/IPM-Net">[code2]</a></li>
<li> One paper to appear at CVPR 2019 as oral presentation. <a href="https://zdzheng.xyz/publication/Joint2019">[PDF]</a><a href="https://www.youtube.com/watch?v=ubCrEAIpQs4">[3-min video]</a> <a href="https://github.com/NVlabs/DG-Net">[code]</a></li>
</ul>
<!---	
<li> One paper to appear at ECCV 2018. <a href="https://arxiv.org/abd/1807.08260">[arXiv]</a> <a href="https://github.com/RoyalVane/MMAN">[code]</a> </li>
<li> One paper to appear at CVPR 2018. <a href="https://arxiv.org/abs/1711.10295">[arXiv]</a> <a href="https://github.com/zhunzhong07/CamStyle">[code]</a> </li>
<li> One paper to appear at ICCV 2017 as spotlight presentation. <a href="https://arxiv.org/abs/1701.07717">[arXiv]</a> <a href="https://github.com/layumi/Person-reID_GAN"> [code]</a> </li>
-->

<hr>

<h2>Others</h2>
<ul>
<li>  <a href="https://www.youtube.com/watch?v=kI3Oc-sxSoA">Shanghai</a> is my hometown, and it is a lovely place to have a sightseeing tour. </li>
<li>  I was a poster maker when I studied at Fudan University. You may check out <a href="https://www.zdzheng.xyz/poster_page">my posters</a>.</li>
<li>  I sometimes write Chinese blogs and share insights at <a href="https://www.zhihu.com/people/zhengzhedong">Zhihu</a>.</li>
</ul>
Do not press the red button!

<a href="https://zdzheng.xyz/redbutton.html"> <img src="https://zdzheng.xyz/images/red.jpeg" alt="red" width="50" height="50"></a>
<a href="https://zdzheng.xyz/greenbutton.html"> <img src="https://zdzheng.xyz/images/green.jpeg" alt="green" width="50" height="50"> </a>

<hr>

<div style='width:600px;height:300px;margin:0 auto'>
<link rel="preconnect" href="//cdn.clustrmaps.com">
<link rel="dns-prefetch" href="//cdn.clustrmaps.com">
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?d=mhnrYabZI2bz_eHk1W_A8VvNxtAjYBrWfIfxbLnTRPQ&cmo=faa659&cl=ffffff&w=a' async></script>

<script type="application/ld+json">
		    { "@context": "https://schema.org", 
		     "@type": "Person",
			 "name": "Zhedong Zheng",
			 "gender": "Male",
			 "Description": "Computer Vision Researcher",
			 "jobTitle": "Research Fellow",
			 "alumniOf": [
			 	{
			 	 "@type": "University",
			 	 "name": "University of Technology Sydney"
			 	},
			 	{
			 	 "@type": "University",
			 	 "name": "Fudan University"
			 	}
			 ],
			 "url": "https://zdzheng.xyz",
			 "image": "https://zdzheng.xyz/images/profile.webp",
			 "sameAs": [
			 	"https://www.researchgate.net/profile/Zhedong-Zheng-2",
				"https://www.facebook.com/zhedongzheng12",
				"https://www.linkedin.com/in/zhedongzheng",
				"https://github.com/layumi",
				"https://scholar.google.com/citations?user=XT17oUEAAAAJ",
				"http://orcid.org/0000-0002-2434-9050",
				"https://www.zhihu.com/people/zhengzhedong"
			 	]
	        }
</script>
</div>




  


